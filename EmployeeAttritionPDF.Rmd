---
title: "Attrition Classification"
author: "Anna Cerbaro, Erica Marras, Eleni Papadopulos"
output: 
  pdf_document:
    toc: true  # Add this line for a table of contents
date: "2022/2023"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem presentation

Companies are often forced to deal with attrition, being the choice of employees to voluntarily leave the workplace. It is intuitive to see how this phenomenon could potentially constitute a problem for any company: in fact, employee turnover can be expensive because recruitment, hiring and training expenses need to be covered and the risk of disrupting the workplace stability and productivity increases. Hence the need to propose a strategy to foresee and, if possible, prevent attrition and possibly mitigate the consequences.

This premise motivates the decision to analyze the "Employee Attrition and Factors" dataset, concerning the reasons that may lead employees to leave the company they work at. We are dealing with a classification problem: our objective is to predict whether someone ultimately leaves their job or not and the motivations behind.

## Dataset presentation

Let us start our dissertation by importing the selected dataset.

```{r include=FALSE}
library(ggplot2)
library(plyr)
library(dplyr)
library(ggpubr)
library(GGally)
library(corrplot)
library(ROSE)
library(caret)
library(ggcorrplot)
library(rsample)
library(car)
library(pROC)
library(class)
library(performanceEstimation)
library(glmnet)
library(e1071)
library(naivebayes)
library(MASS)
library(FactoMineR)
library(ROCR)
library(PRROC)
library(knitr)
library(lessR)
library(rcompanion)
library(ggplot2)
library(plyr)
library(dplyr)
library(ggpubr)
library(GGally)
library(ggridges)
library(corrplot)
library(ROSE)
library(caret)
library(ggcorrplot)
library(rsample)
library(car)
library(pROC)
#install.packages("lessR")
library(lessR)
library(rcompanion)
```

```{r}
HR_Analytics <- read.csv("~/Downloads/HR_Analytics.csv")
attach(HR_Analytics)
```


```{r}
HR_Analytics <- HR_Analytics[, -c(4, 13)]
```

The chosen dataset is synthetically generated by IBM. It consists of 1470 instances, each corresponding to an employee of the company and described by 33 features, both numerical and categorical, consisting in the factors that may cause attrition. The column *Attrition* is a logical variable that indicates whether an employee has quit.

| Name of the variable     | Type       | Description                                                              |
|-------------------|-------------------|----------------------------------|
| Age                      | Discrete   | The age of the employee                                                  |
| Attrition                | Nominal    | Whether or not the employee has left the organization                    |
| BusinessTravel           | Ordinal    | The frequency of business travel for the employee                        |
| Department               | Nominal    | The department the employee works in                                     |
| DistanceFromHome         | Discrete   | The distance from home in miles for the employee                         |
| Education                | Ordinal    | The level of education achieved by the employee                          |
| EducationField           | Nominal    | The field of study for the employee's education                          |
| EmployeeCount            | Discrete   | The total number of employees in the organization                        |
| EmployeeNumber           | Discrete   | A unique identifier for each employee profile                            |
| EnvironmentSatisfaction  | Ordinal    | The employee's satisfaction with their work environment                  |
| Gender                   | Nominal    | The gender of the employee                                               |
| JobInvolvement           | Ordinal    | The level of involvement required for the employee's job                 |
| JobLevel                 | Ordinal    | The job level of the employee                                            |
| JobRole                  | Nominal    | The role of the employee in the organization                             |
| JobSatisfaction          | Ordinal    | The employee's satisfaction with their job                               |
| MaritalStatus            | Nominal    | The marital status of the employee                                       |
| MonthlyIncome            | Continuous | The monthly income of the employee                                       |
| MonthlyRate              | Continous  | The monthly rate of pay for the employee                                 |
| NumCompaniesWorked       | Discrete   | The number of companies the employee has worked for                      |
| Over18                   | Nominal    | Whether or not the employee is over 18                                   |
| OverTime                 | Nominal    | Whether or not the employee works overtime                               |
| PercentSalaryHike        | Discrete   | The percentage of salary hike for the employee                           |
| PerformanceRating        | Ordinal    | The performance rating of the employee                                   |
| RelationshipSatisfaction | Ordinal    | The employee's satisfaction with their relationships                     |
| StandardHours            | Discrete   | The standard hours of work for the employees                             |
| StockOptionLevel         | Ordinal    | The stock option level of the employee                                   |
| TotalWorkingYears        | Discrete   | The total number of years the employee has worked                        |
| TrainingTimesLastYear    | Discrete   | The number of times the employee was taken for training in the last year |
| WorkLifeBalance          | Ordinal    | The employee's perception of their work-life balance                     |
| YearsAtCompany           | Discrete   | The number of years the employee has been with the company               |
| YearsInCurrentRole       | Discrete   | The number of years the employee has been in their current role          |
| YearsSinceLastPromotion  | Discrete   | The number of years since the employee's last promotion                  |
| YearsWithCurrManager     | Discrete   | The number of years the employee has been with their current manager     |

## Data preprocessing

Our data analysis begins with the search for null values and missing data:

```{r}
sum(is.na.data.frame(HR_Analytics))
sum(duplicated(HR_Analytics))
```

Taking a closer look to the features, we observed:

-   the presence of the column *EmployeeNumber* that does not give any substantial contribute to the analysis, as it is merely used to keep count of the employees;

-   the presence of the columns *Over18*, *EmployeeCount*, *StandardHours* that only take a single value each, making them uninformative. For example:

```{r}
unique(Over18)
```

-   inconsistency with the column *NumCompaniesWorked* that contains some 0 values. It is clear from the example below that such scenario is not possible (the employee must have worked at at least 2 companies, yet the first columns says otherwise). Therefore, we conclude that there may have been a data collection error.

```{r}
HR_Analytics[6,c("NumCompaniesWorked", "TotalWorkingYears", "YearsAtCompany")]
```

Due to these reasons, we decide to remove the columns *EmployeeNumber*, *Over18*, *EmployeeCount* and *StandardHours* from our dataset. Additionaly, we removed any instances where the *NumCompaniesWorked* value was equal to 0.

```{r}
HR_Analytics <- HR_Analytics[NumCompaniesWorked!= 0,]
HR_Analytics <- HR_Analytics[, -c(8, 9, 18, 25, 20)]
```

## Exploratory Data Analysis

We now want to figure out how the features in the HR_Analytics dataset are distributed. Firstly, we take a look at the summary of it.


```{r}
summary(HR_Analytics)
```

```{r include=FALSE}
custom_colors <- c('#64804c','#684c80')
custom_colorsA <- c('#087815','#78086b')
custom_colorsB <- c('#547658','#765472')
custom_colorsC <- c('#4C8068','#804C64')
custom_colorsD <- c('#68b7bd','#8f5a87')

palette <- c('#E0794A','#D96D5C','#D1616D','#BD5E79','#A95A85','#8F5A87','#755988','#5e567f','#465376')
```

## Description of variables:


```{r}
table(HR_Analytics$Attrition)
```

The distribution of the **target variable** *Attrition* is unbalanced, with approximately 84% of employees staying at the company and the remaining 16% leaving.

*Nominal categorical variables*

```{r echo=FALSE, fig.height=2, fig.align='left'}
tb <- table(HR_Analytics$Attrition)
percent <- prop.table(tb)*100


ggplot(data = data.frame(percent), aes(x = "", y = Freq, fill = Var1)) +
  geom_bar(stat = "identity", width = 1, col='black') +
  coord_polar(theta = "y") +
  theme_void() +
  labs(title = "Attrition Distribution", fill=" ") +
  geom_text(aes(label = sprintf("%.1f%%", Freq)), 
            position = position_stack(vjust = 0.5), size=2.5)+
  scale_fill_manual(values = custom_colorsD)
```

```{r, echo=FALSE, fig.align='left', fig.width=8, fig.height=7}

tb1 <- table(HR_Analytics$BusinessTravel)
percent1 <- prop.table(tb1)*100

tb2 <- table(HR_Analytics$Department)
percent2 <- prop.table(tb2)*100

tb3 <- table(HR_Analytics$EducationField)
percent3 <- prop.table(tb3)*100

tb4 <- table(HR_Analytics$Gender)
percent4 <- prop.table(tb4)*100

tb5 <- table(HR_Analytics$MaritalStatus)
percent5 <- prop.table(tb5)*100

tb6 <- table(HR_Analytics$OverTime)
percent6 <- prop.table(tb6)*100

tb7 <- table(HR_Analytics$JobRole)
percent7 <- prop.table(tb7)*100

ggarrange(
  ggplot(data = data.frame(percent1), aes(x = "", y = Freq, fill = Var1)) +
  geom_bar(stat = "identity", width = 1, col='black') +
  coord_polar(theta = "y") +
  theme_void() +
  labs(title = "Business Travel Distribution", fill=" ") +
  geom_text(aes(label = sprintf("%.1f%%", Freq)), 
            position = position_stack(vjust = 0.5), size=1.7)+
  scale_fill_manual(values = palette),
ggplot(data = data.frame(percent2), aes(x = "", y = Freq, fill = Var1)) +
  geom_bar(stat = "identity", width = 1, col='black') +
  coord_polar(theta = "y") +
  theme_void() +
  labs(title = "Department Distribution", fill=" ") +
  geom_text(aes(label = sprintf("%.1f%%", Freq)), 
            position = position_stack(vjust = 0.5), size=1.7)+
  scale_fill_manual(values = palette),
ggplot(data = data.frame(percent7), aes(x = "", y = Freq, fill = Var1)) +
  geom_bar(stat = "identity", width = 1, col='black') +
  coord_polar(theta = "y") +
  theme_void() +
  labs(title = "Job Role Distribution", fill=" ") +
  geom_text(aes(label = sprintf("%.1f%%", Freq)), 
            position = position_stack(vjust = 0.5), size=1.7)+
  scale_fill_manual(values = palette)+
  theme(plot.title = element_text(hjust=0.5)),

ggplot(data = data.frame(percent3), aes(x = "", y = Freq, fill = Var1)) +
  geom_bar(stat = "identity", width = 1, col='black') +
  coord_polar(theta = "y") +
  theme_void() +
  labs(title = "Education Field Distribution", fill=" ") +
  geom_text(aes(label = sprintf("%.1f%%", Freq)), 
            position = position_stack(vjust = 0.5), size=1.7)+
  scale_fill_manual(values = palette),
  
ncol = 2,
nrow = 2
)
```

```{r, echo=FALSE, fig.align='left', fig.width=8, fig.height=6}
ggarrange(
  ggplot(data = data.frame(percent4), aes(x = "", y = Freq, fill = Var1)) +
  geom_bar(stat = "identity", width = 1, col='black') +
  coord_polar(theta = "y") +
  theme_void() +
  labs(title = "Gender Distribution", fill=" ") +
  geom_text(aes(label = sprintf("%.1f%%", Freq)), 
            position = position_stack(vjust = 0.5), size=1.7)+
  scale_fill_manual(values = c(palette[1], palette[3])),
ggplot(data = data.frame(percent5), aes(x = "", y = Freq, fill = Var1)) +
  geom_bar(stat = "identity", width = 1, col='black') +
  coord_polar(theta = "y") +
  theme_void() +
  labs(title = "Marital Status Distribution", fill=" ") +
  geom_text(aes(label = sprintf("%.1f%%", Freq)), 
            position = position_stack(vjust = 0.5), size=1.7)+
  scale_fill_manual(values = palette),

ggplot(data = data.frame(percent6), aes(x = "", y = Freq, fill = Var1)) +
  geom_bar(stat = "identity", width = 1, col='black') +
  coord_polar(theta = "y") +
  theme_void() +
  labs(title = "Over Time Distribution", fill=" ") +
  geom_text(aes(label = sprintf("%.1f%%", Freq)), 
            position = position_stack(vjust = 0.5), size=1.7)+
  scale_fill_manual(values=c(palette[1], palette[3])),
ncol = 2,
nrow = 2
)
```

*Ordinal variables*

\
                
```{r, echo=FALSE, fig.align='left', fig.height=14, fig.width=18, message=FALSE}
ggarrange(
    ggplot(HR_Analytics, aes(x=Education)) + 
  geom_bar(colour="black", fill=palette[4], alpha=0.8)+
  theme_minimal(),
    ggplot(HR_Analytics, aes(x=JobLevel)) + 
  geom_bar(colour="black", fill=palette[4], alpha=0.8)+
  theme_minimal(),
    ggplot(HR_Analytics, aes(x=StockOptionLevel)) + 
  geom_bar(colour="black", fill=palette[4], alpha=0.8)+
  theme_minimal(),
      ggplot(HR_Analytics, aes(x=PerformanceRating)) + 
  geom_bar(colour="black", fill=palette[4], alpha=0.8)+
  theme_minimal(),
    ggplot(HR_Analytics, aes(x= JobInvolvement)) + 
  geom_bar(colour="black", fill=palette[4], alpha=0.8)+
  theme_minimal(),
      ggplot(HR_Analytics, aes(x=JobSatisfaction)) + 
  geom_bar(colour="black", fill=palette[4], alpha=0.8)+
  theme_minimal(),
        ggplot(HR_Analytics, aes(x=EnvironmentSatisfaction)) + 
  geom_bar(colour="black", fill=palette[4], alpha=0.8)+
  theme_minimal(),
      ggplot(HR_Analytics, aes(x=RelationshipSatisfaction)) + 
  geom_bar(colour="black", fill=palette[4], alpha=0.8)+
  theme_minimal(),
        ggplot(HR_Analytics, aes(x=WorkLifeBalance)) + 
  geom_bar(colour="black", fill=palette[4], alpha=0.8)+
  theme_minimal(),
  
ncol=2,
nrow=5
)
```

*Numerical (continuous and discrete) variables*

\
```{r, echo=FALSE, fig.height=6, fig.width=22, message=FALSE, fig.align='left'}
ggarrange(
  ggplot(HR_Analytics, aes(x=MonthlyIncome)) + 
  geom_histogram(aes(y=..density..), colour="black", fill=palette[9], alpha=0.7)+
  geom_density(color = 'black', fill = palette[2], alpha = 0.3) +
  geom_vline(data=HR_Analytics, aes(xintercept=mean(MonthlyIncome)),
             linetype="dashed", colour="darkred") +
  theme_minimal(),
ggplot(HR_Analytics, aes(x = MonthlyIncome)) + 
  stat_boxplot(geom = "errorbar", width = 0.15, color = 'black') + 
  geom_boxplot(fill = palette[1], alpha = 0.7, color = 'black',         
               outlier.colour = palette[2])+
  theme_minimal(),
nrow=1,
ncol=2
)
```

We observe that the outliers in MonthlyIncome are entirely justifiable, as they correspond to higher-level positions (such as Research Director and Manager) with significant responsibilities, resulting in higher earnings.

```{r echo=FALSE, fig.height=2, fig.width=6, message=FALSE}
ggplot(HR_Analytics, aes(x = MonthlyIncome, y = JobRole)) +
  geom_density_ridges(color = 'black', fill = palette[1], alpha=0.7) +
  scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
  scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
  theme_minimal()
```

```{r, echo=FALSE, fig.height=15, fig.width=22, message=FALSE, fig.align='left'}
ggarrange(
  ggplot(HR_Analytics, aes(x=Age)) + 
  geom_histogram(aes(y=..density..), colour="black", fill=palette[9], alpha=0.7)+
  theme_minimal(),
ggplot(HR_Analytics, aes(x = Age)) + 
  stat_boxplot(geom = "errorbar", width = 0.15, color = 'black') + 
  geom_boxplot(fill = palette[1], alpha = 0.7, color = 'black',         
               outlier.colour = palette[2])+
  theme_minimal(),

ggplot(HR_Analytics, aes(x=DistanceFromHome)) + 
  geom_histogram(aes(y=..density..), colour="black", fill=palette[9], alpha=0.7)+
  theme_minimal(),
ggplot(HR_Analytics, aes(x = DistanceFromHome)) + 
  stat_boxplot(geom = "errorbar", width = 0.15, color = 'black') + 
  geom_boxplot(fill = palette[1], alpha = 0.7, color = 'black',         
               outlier.colour = palette[2])+
  theme_minimal(),
ncol=2,
nrow=2
)

```

```{r, echo=FALSE, fig.height=20, fig.width=22, message=FALSE, fig.align='left'}
ggarrange(
  
    ggplot(HR_Analytics, aes(x=PercentSalaryHike)) + 
  geom_bar(colour="black", fill=palette[9], alpha=0.7)+
  theme_minimal(),
ggplot(HR_Analytics, aes(x = PercentSalaryHike)) + 
  stat_boxplot(geom = "errorbar", width = 0.15, color = 'black') + 
  geom_boxplot(fill = palette[1], alpha = 0.7, color = 'black',         
               outlier.colour = palette[2])+
  theme_minimal(),
    ggplot(HR_Analytics, aes(x=YearsAtCompany)) + 
  geom_bar(colour="black", fill=palette[9], alpha=0.7)+
  theme_minimal(),
ggplot(HR_Analytics, aes(x = YearsAtCompany)) + 
  stat_boxplot(geom = "errorbar", width = 0.15, color = 'black') + 
  geom_boxplot(fill = palette[1], alpha = 0.7, color = 'black',         
               outlier.colour = palette[2])+
  theme_minimal(),
    ggplot(HR_Analytics, aes(x=TotalWorkingYears)) + 
  geom_bar(colour="black", fill=palette[9], alpha=0.7)+
  theme_minimal() ,
ggplot(HR_Analytics, aes(x = TotalWorkingYears)) + 
  stat_boxplot(geom = "errorbar", width = 0.15, color = 'black') + 
  geom_boxplot(fill = palette[1], alpha = 0.7, color = 'black',         
               outlier.colour = palette[2])+
  theme_minimal(),
ncol=2,
nrow=3
)

```

```{r, echo=FALSE, fig.height=33, fig.width=20, message=FALSE, fig.align='left'}
ggarrange(

    ggplot(HR_Analytics, aes(x=YearsInCurrentRole)) + 
  geom_bar(colour="black", fill=palette[9], alpha=0.7)+
  theme_minimal(),
ggplot(HR_Analytics, aes(x = YearsInCurrentRole)) + 
  stat_boxplot(geom = "errorbar", width = 0.15, color = 'black') + 
  geom_boxplot(fill = palette[1], alpha = 0.7, color = 'black',         
               outlier.colour = palette[2])+
  theme_minimal(),
    ggplot(HR_Analytics, aes(x=YearsWithCurrManager)) + 
  geom_bar(colour="black", fill=palette[9], alpha=0.7)+
  theme_minimal(),
ggplot(HR_Analytics, aes(x = YearsWithCurrManager)) + 
  stat_boxplot(geom = "errorbar", width = 0.15, color = 'black') + 
  geom_boxplot(fill = palette[1], alpha = 0.7, color = 'black',         
               outlier.colour = palette[2])+
  theme_minimal(),
    ggplot(HR_Analytics, aes(x=YearsSinceLastPromotion)) + 
  geom_bar(colour="black", fill=palette[9], alpha=0.7)+
  theme_minimal(),
ggplot(HR_Analytics, aes(x = YearsSinceLastPromotion)) + 
  stat_boxplot(geom = "errorbar", width = 0.15, color = 'black') + 
  geom_boxplot(fill = palette[1], alpha = 0.7, color = 'black',         
               outlier.colour = palette[2])+
  theme_minimal(),
  ggplot(HR_Analytics, aes(x=NumCompaniesWorked)) + 
  geom_bar(colour="black", fill=palette[9], alpha=0.7)+
  theme_minimal(),
ggplot(HR_Analytics, aes(x = NumCompaniesWorked)) + 
  stat_boxplot(geom = "errorbar", width = 0.15, color = 'black') + 
  geom_boxplot(fill = palette[1], alpha = 0.7, color = 'black',         
               outlier.colour = palette[2])+
  theme_minimal(),
  ggplot(HR_Analytics, aes(x=TrainingTimesLastYear)) + 
  geom_bar(colour="black", fill=palette[9], alpha=0.7)+
  theme_minimal(),
ggplot(HR_Analytics, aes(x = TrainingTimesLastYear)) + 
  stat_boxplot(geom = "errorbar", width = 0.15, color = 'black') + 
  geom_boxplot(fill = palette[1], alpha = 0.7, color = 'black',         
               outlier.colour = palette[2])+
  theme_minimal(),
ncol=2,
nrow=5
)

```

#### Inspection of distributions concerning Attrition

\
We want to explore the distribution of all the features concerning *Attrition* to obtain a general overview and formulate meaningful questions to identify potential patterns.

```{r echo=FALSE, fig.height=3, message=FALSE, fig.align='left'}
ggarrange(
  ggplot(HR_Analytics) + geom_bar(aes(x = BusinessTravel, fill=Attrition), position = 'fill') +
  scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
  theme_minimal()+
    theme(axis.text.x = element_text(angle = 45, hjust = 1)),

  ggplot(HR_Analytics) + geom_bar(aes(x = Department, fill=Attrition), position = 'fill') +
  scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)),

  (ggplot(HR_Analytics) + geom_bar(aes(x = JobRole, fill=Attrition), position = 'fill') +
  scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) ),

ncol = 3,
nrow=1,
common.legend = TRUE
  )
```

```{r echo=FALSE, fig.height=6, message=FALSE, fig.align='left'}
ggarrange(

ggplot(HR_Analytics) + geom_bar(aes(x = EducationField, fill=Attrition), position = 'fill') +
  scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)),
ggplot(HR_Analytics) + geom_bar(aes(x = Gender, fill=Attrition), position = 'fill') +
  scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)),
(ggplot(HR_Analytics) + geom_bar(aes(x = MaritalStatus, fill=Attrition), position = 'fill') +
  scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
  theme_minimal())+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)),
(ggplot(HR_Analytics) + geom_bar(aes(x = OverTime, fill=Attrition), position = 'fill') +
  scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) ),

ncol = 3,
nrow=2,
common.legend = TRUE
  )
```

```{r echo=FALSE, fig.height=7, message=FALSE, fig.align='left'}
ggarrange(ggplot(HR_Analytics) + geom_bar(aes(x = Education, fill=Attrition), position = 'fill') +
  scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
  theme_minimal()+
    theme(axis.text.x = element_text(angle = 0, hjust = 1)),
ggplot(HR_Analytics) + geom_bar(aes(x = StockOptionLevel, fill=Attrition), position = 'fill') +
  scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 0, hjust = 1)),

ggplot(HR_Analytics) + geom_bar(aes(x = PerformanceRating, fill=Attrition), position = 'fill') +
  scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 0, hjust = 1)),

ggplot(HR_Analytics) + geom_bar(aes(x = JobInvolvement, fill=Attrition), position = 'fill') +
  scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 0, hjust = 1)),
(ggplot(HR_Analytics) + geom_bar(aes(x = JobSatisfaction, fill=Attrition), position = 'fill') +
  scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
  theme_minimal())+
  theme(axis.text.x = element_text(angle = 0, hjust = 1)),
(ggplot(HR_Analytics) + geom_bar(aes(x = EnvironmentSatisfaction, fill=Attrition), position = 'fill') +
  scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 0, hjust = 1)) ),
(ggplot(HR_Analytics) + geom_bar(aes(x = RelationshipSatisfaction, fill=Attrition), position = 'fill') +
  scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
  theme_minimal())+
  theme(axis.text.x = element_text(angle = 0, hjust = 1)),
ncol = 3,
nrow=3,
common.legend = TRUE
  )
```

```{r, echo=FALSE, fig.height=6, fig.width=22, message=FALSE, fig.align='left'}
ggarrange(
  
  ggplot(HR_Analytics) + geom_density(aes(x = MonthlyIncome, fill=Attrition), alpha=0.8) +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  
  ggplot(HR_Analytics, aes(x = MonthlyIncome, fill=Attrition)) + 
    stat_boxplot(geom = "errorbar", width = 0.75, color = 'black') + 
    geom_boxplot(color='black') +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  nrow=1,
  ncol=2,
  common.legend = TRUE
)
```

```{r, echo=FALSE, fig.height=6, fig.width=22, message=FALSE, fig.align='left'}
ggarrange(
  
  ggplot(HR_Analytics) + geom_histogram(aes(x = Age, fill=Attrition), alpha=1, colour='black', bins=30) +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  
  ggplot(HR_Analytics, aes(x = Age, fill=Attrition)) + 
    stat_boxplot(geom = "errorbar", width = 0.75, color = 'black') + 
    geom_boxplot(color='black') +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  nrow=1,
  ncol=2,
  common.legend = TRUE
)
```

```{r, echo=FALSE, fig.height=6, fig.width=22, message=FALSE, fig.align='left'}
ggarrange(
  
  ggplot(HR_Analytics) + geom_histogram(aes(x = DistanceFromHome, fill=Attrition), alpha=1, colour='black', bins=30) +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  
  ggplot(HR_Analytics, aes(x = DistanceFromHome, fill=Attrition)) + 
    stat_boxplot(geom = "errorbar", width = 0.75, color = 'black') + 
    geom_boxplot(color='black') +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  nrow=1,
  ncol=2,
  common.legend = TRUE
)
```

```{r, echo=FALSE, fig.height=6, fig.width=22, message=FALSE, fig.align='left'}
ggarrange(
  
  ggplot(HR_Analytics) + geom_histogram(aes(x = PercentSalaryHike, fill=Attrition), alpha=1, colour='black', bins=30) +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  
  ggplot(HR_Analytics, aes(x = PercentSalaryHike, fill=Attrition)) + 
    stat_boxplot(geom = "errorbar", width = 0.75, color = 'black') + 
    geom_boxplot(color='black') +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  nrow=1,
  ncol=2,
  common.legend = TRUE
)
```

```{r, echo=FALSE, fig.height=6, fig.width=22, message=FALSE, fig.align='left'}
ggarrange(
  
  ggplot(HR_Analytics) + geom_histogram(aes(x = YearsAtCompany, fill=Attrition), alpha=1, colour='black', bins=30) +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  
  ggplot(HR_Analytics, aes(x = YearsAtCompany, fill=Attrition)) + 
    stat_boxplot(geom = "errorbar", width = 0.75, color = 'black') + 
    geom_boxplot(color='black') +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  nrow=1,
  ncol=2,
  common.legend = TRUE
)
```

```{r, echo=FALSE, fig.height=6, fig.width=22, message=FALSE, fig.align='left'}
ggarrange(
  
  ggplot(HR_Analytics) + geom_histogram(aes(x = TotalWorkingYears, fill=Attrition), alpha=1, colour='black', bins=30) +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  
  ggplot(HR_Analytics, aes(x = TotalWorkingYears, fill=Attrition)) + 
    stat_boxplot(geom = "errorbar", width = 0.75, color = 'black') + 
    geom_boxplot(color='black') +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  nrow=1,
  ncol=2,
  common.legend = TRUE
)
```

```{r, echo=FALSE, fig.height=6, fig.width=22, message=FALSE, fig.align='left'}
ggarrange(
  
  ggplot(HR_Analytics) + geom_histogram(aes(x = YearsInCurrentRole, fill=Attrition), alpha=1, colour='black', bins=30) +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  
  ggplot(HR_Analytics, aes(x = YearsInCurrentRole, fill=Attrition)) + 
    stat_boxplot(geom = "errorbar", width = 0.75, color = 'black') + 
    geom_boxplot(color='black') +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  nrow=1,
  ncol=2,
  common.legend = TRUE
)
```

```{r, echo=FALSE, fig.height=6, fig.width=22, message=FALSE, fig.align='left'}
ggarrange(
  
  ggplot(HR_Analytics) + geom_histogram(aes(x = YearsWithCurrManager, fill=Attrition), alpha=1, colour='black', bins=30) +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  
  ggplot(HR_Analytics, aes(x = YearsWithCurrManager, fill=Attrition)) + 
    stat_boxplot(geom = "errorbar", width = 0.75, color = 'black') + 
    geom_boxplot(color='black') +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  nrow=1,
  ncol=2,
  common.legend = TRUE
)
```

```{r, echo=FALSE, fig.height=6, fig.width=22, message=FALSE, fig.align='left'}
ggarrange(
  
  ggplot(HR_Analytics) + geom_histogram(aes(x = YearsSinceLastPromotion, fill=Attrition), alpha=1, colour='black', bins=30) +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  
  ggplot(HR_Analytics, aes(x = YearsSinceLastPromotion, fill=Attrition)) + 
    stat_boxplot(geom = "errorbar", width = 0.75, color = 'black') + 
    geom_boxplot(color='black') +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  nrow=1,
  ncol=2,
  common.legend = TRUE
)
```

```{r, echo=FALSE, fig.height=6, fig.width=22, message=FALSE, fig.align='left'}
ggarrange(
  
  ggplot(HR_Analytics) + geom_histogram(aes(x = NumCompaniesWorked, fill=Attrition), alpha=1, colour='black', bins=30) +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  
  ggplot(HR_Analytics, aes(x = NumCompaniesWorked, fill=Attrition)) + 
    stat_boxplot(geom = "errorbar", width = 0.75, color = 'black') + 
    geom_boxplot(color='black') +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  nrow=1,
  ncol=2,
  common.legend = TRUE
)
```

```{r, echo=FALSE, fig.height=6, fig.width=22, message=FALSE, fig.align='left'}
ggarrange(
  
  ggplot(HR_Analytics) + geom_histogram(aes(x = TrainingTimesLastYear, fill=Attrition), alpha=1, colour='black', bins=30) +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  
  ggplot(HR_Analytics, aes(x = TrainingTimesLastYear, fill=Attrition)) + 
    stat_boxplot(geom = "errorbar", width = 0.75, color = 'black') + 
    geom_boxplot(color='black') +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  nrow=1,
  ncol=2,
  common.legend = TRUE
)
```

#### Correlation

\
To better understand the relationships among the numerical variables in the dataset, we aim to visualize these relationships by creating a correlation matrix.

```{r, echo=FALSE, fig.align='left'}
corr_data <- HR_Analytics[, c(1, 5, 6, 15, 16, 22, 23, 24, 25, 26, 27, 28)]
corr <- cor(corr_data)

ggcorrplot(corr, 
           hc.order = TRUE, 
           type = "upper", 
           lab=TRUE, 
           lab_size = 3,
           tl.cex=6,
           colors=c(palette[9],'#f9e5dc',palette[4]))
```

Here Cramer's V correlation among categorical (nominal and ordinal) variables with respect to the target variable *Attrition*.

```{r echo=FALSE}
cat("                      CRAMER'S V \n")
paste("Attrition-BusinessTravel          ", cramerV(table(HR_Analytics$Attrition, HR_Analytics$BusinessTravel)))
paste("Attrition-Department              ",cramerV(table(HR_Analytics$Attrition, HR_Analytics$Department)))
paste("Attrition-EducationField          ",cramerV(table(HR_Analytics$Attrition, HR_Analytics$EducationField)))
paste("Attrition-Gender                  ",cramerV(table(HR_Analytics$Attrition, HR_Analytics$Gender)))
paste("Attrition-MaritalStatus           ",cramerV(table(HR_Analytics$Attrition, HR_Analytics$MaritalStatus)))
paste("Attrition-OverTime                ",cramerV(table(HR_Analytics$Attrition, HR_Analytics$OverTime)))
paste("Attrition-JobRole                 ",cramerV(table(HR_Analytics$Attrition, HR_Analytics$JobRole)))

paste("Attrition-Education               ",cramerV(table(HR_Analytics$Attrition, HR_Analytics$Education)))
paste("Attrition-JobLevel                ",cramerV(table(HR_Analytics$Attrition, HR_Analytics$JobLevel)))
paste("Attrition-StockOptionLevel        ",cramerV(table(HR_Analytics$Attrition, HR_Analytics$StockOptionLevel)))
paste("Attrition-PerformanceRating       ",cramerV(table(HR_Analytics$Attrition, HR_Analytics$PerformanceRating)))
paste("Attrition-JobInvolvement          ",cramerV(table(HR_Analytics$Attrition, HR_Analytics$JobInvolvement)))
paste("Attrition-JobSatisfaction         ",cramerV(table(HR_Analytics$Attrition, HR_Analytics$JobSatisfaction)))
paste("Attrition-EnvironmentSatisfaction ",cramerV(table(HR_Analytics$Attrition, HR_Analytics$EnvironmentSatisfaction)))
paste("Attrition-RelationshipSatisfaction",cramerV(table(HR_Analytics$Attrition, HR_Analytics$RelationshipSatisfaction)))
```

The variables most strongly correlated with *Attrition* are *OverTime*, *JobRole*, *JobLevel*, *StockOptionLevel*.

## Questions

1.  **What are the most common characteristics exhibited by leaving employees?**

```{r echo=FALSE, fig.height=3}
  ggarrange(
  ggplot(HR_Analytics, aes(y = Age, fill=Attrition)) + 
    geom_boxplot(alpha = 1, color = 'black')+
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
    ggplot(HR_Analytics, aes(y=Age, x=Gender, fill=Attrition)) + 
    scale_fill_manual(values=custom_colorsD) +
    geom_boxplot() +
    theme_minimal(),
  ncol=2,
  nrow=1,
  common.legend = TRUE
  )
```

From the box plots, it's evident that employees who leave the company tend to be younger than their counterparts who choose to stay. This aligns with recent studies indicating that the 'Z' and 'Millennial' generations are more inclined to change jobs frequently compared to previous generations. Additionally, we've noticed that among those who leave, female employees are generally younger than their male colleagues. This led us to wonder whether there's an imbalanced gender distribution between those who leave and those who stay. However, based on the following plot, it's clear that gender is not a defining factor in shaping the typical profile of those who decide to leave.

```{r echo=FALSE, fig.height=1, fig.height=3,message=FALSE}

ggplot(HR_Analytics) + geom_bar(aes(y = Attrition, group=Gender, fill=Gender), col='black', position = 'fill', alpha=0.9) +
    theme_minimal()+
    scale_fill_manual(values=c(palette[3],palette[9])) +
    theme(axis.text.x = element_text(angle = 0, hjust = 1))
```

We're interested in examining how marital status is distributed among employees in the context of attrition. It's worth highlighting that a substantial 50% of those who choose to leave the company are single employees.

```{r echo=FALSE, fig.height=1, fig.height=3, message=FALSE}

ggplot(HR_Analytics) + geom_bar(aes(y = Attrition, group=MaritalStatus, fill=MaritalStatus), col='black', position = 'fill', alpha=0.9) +
      theme_minimal()+
      scale_fill_manual(values=c(palette[1],palette[4], palette[2])) +
      theme(axis.text.x = element_text(angle = 0, hjust = 1))
```

Let's create a visualization that considers both age and other factors to gain a deeper insight into the typical profile of employees who decide to leave.

```{r echo=FALSE, fig.height=3, message=FALSE, warning=FALSE}
age_ranges <- c(17, 20, 24, 30, 36, 44, 50, 60)
age_labels <- c("18-20","21-24", "25-30", "31-36", "37-44","45-50", "51-60" ) 

HR_Analytics$AgeIntervals <- cut(HR_Analytics$Age, breaks = age_ranges, labels = age_labels)

percentage_data <- HR_Analytics %>%
      group_by(AgeIntervals, MaritalStatus) %>%
      summarize(Attrition_Percentage = mean(Attrition == "Yes") * 100)

ggplot(HR_Analytics, aes(AgeIntervals, MaritalStatus)) + 
      geom_jitter(aes(color = Attrition), size = 2, alpha=0.3) +
      scale_fill_manual(values=custom_colorsD) +
      scale_colour_manual(values=custom_colorsD) +
      theme_minimal() +
      labs(x="Intervals of Age") +
      geom_text(data = percentage_data,
             aes(label = paste0(round(Attrition_Percentage, 1), "%")),
             size = 3, color = "#6B3854", fontface = "bold", check_overlap = FALSE)
```

Single employees, particularly those in the younger age groups (under 30), demonstrate a greater likelihood of leaving the company when compared to individuals with different marital statuses.

Now, we want to visualize the distribution of BusinessTravel among the two Attrition classes.

    ```{r echo=FALSE, fig.height=3, message=FALSE}

        ggplot(HR_Analytics) + geom_bar(aes(y = Attrition, group=BusinessTravel, fill=BusinessTravel), col='black', position = 'fill', alpha=0.9) +
          theme_minimal()+
          scale_fill_manual(values=c(palette[3],palette[5], palette[8])) +
          theme(axis.text.x = element_text(angle = 0, hjust = 1))
    ```

It appears that there is a higher presence of employees who travel frequently in the Attrition=Yes class compared to the opposite class (30% vs 15%).

```{r echo=FALSE, fig.height=3, message=FALSE}
percentage_data <- HR_Analytics %>%
    group_by(BusinessTravel, MaritalStatus) %>%
    summarize(Attrition_Percentage = mean(Attrition == "Yes") * 100)

ggplot(HR_Analytics, aes(BusinessTravel, MaritalStatus)) + 
    geom_jitter(aes(color = Attrition), size = 2, alpha=0.5) +
    scale_fill_manual(values=custom_colorsD) +
    scale_colour_manual(values=custom_colorsD) +
    theme_minimal() +
    geom_text(data = percentage_data,
              aes(label = paste0(round(Attrition_Percentage, 1), "%")),
              size = 3, color = "black", fontface = "bold")
```

    
We can observe that, among the groups represented, the highest percentage of attrition is among single employees who travel frequently, followed by single employees who travel rarely. Divorced employees who travel frequently also show a notable percentage of attrition.


2.  **How does the employee well-being impact the decision to leave?**

In the variables *Job Satisfaction*, *Environment Satisfaction*, *Relationship Satisfaction*, *Work-Life Balance*, and *Job Involvement*, we find indicators of employee well-being, contentment, and comfort within the company. Given this, and assuming that these variables may play a role in the decision to leave, we aim to investigate whether there is a significant relationship between each of these variables and the "Attrition" variable.

Before exploring this analysis, we want to provide an initial overview of the overall employee satisfaction among those who choose to stay compared to those who decide to leave. To do this, we calculate the averages of the HR_Analytics Satisfaction variables (Job, Environment, Relationship) and examine their distributions.


```{r echo=FALSE, fig.height=3, message=FALSE}
HR_Analytics$AverageSatisfaction <- (HR_Analytics$JobSatisfaction + HR_Analytics$RelationshipSatisfaction + HR_Analytics$EnvironmentSatisfaction) / 3

ggplot(HR_Analytics, aes(y = Attrition, x = AverageSatisfaction, fill=Attrition)) +
    geom_boxplot() +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal()
```

Both boxplots share an identical median; however, there is a notable contrast in the spread of the data, evident from the second and third quartiles. This indicates that individuals who leave tend to have an overall lower satisfaction level.

To evaluate the relationships and dependencies among the mentioned features (*Job Satisfaction*, *Environment Satisfaction*, *Relationship Satisfaction*, *Work-Life Balance*, *and Job Involvement*), we conducted a statistical analysis using the chi-square test. The chi-square test is a robust tool for examining associations between categorical variables and helps us determine whether there is a significant statistical relationship between these variables and Attrition. Here, we present the p-value results.

    ```{r, echo=FALSE}
    chi_square_resultJS <- chisq.test(table(HR_Analytics$Attrition, HR_Analytics$JobSatisfaction))
    chi_square_resultES <- chisq.test(table(HR_Analytics$Attrition, HR_Analytics$EnvironmentSatisfaction))
    chi_square_resultRS <- chisq.test(table(HR_Analytics$Attrition, HR_Analytics$RelationshipSatisfaction))
    chi_square_resultWLB <- chisq.test(table(HR_Analytics$Attrition, HR_Analytics$WorkLifeBalance))
    chi_square_resultJI <- chisq.test(table(HR_Analytics$Attrition, HR_Analytics$JobInvolvement))



    print(paste("Attrition-JobSatisfaction           p-value <", chi_square_resultJS$p.value ))
    print(paste("Attrition-EnvironmentSatisfaction   p-value <", chi_square_resultES$p.value ))
    print(paste("Attrition-RelationshipSatisfaction  p-value <", chi_square_resultRS$p.value ))
    print(paste("Attrition-WorkLifeBalance           p-value <", chi_square_resultWLB$p.value ))
    print(paste("Attrition-JobInvolvement            p-value <", chi_square_resultJI$p.value ))

    ```

So, for all the considered variables (except for *RelationshipSatisfaction*) the p-value is very small (far from 0.05), suggesting that we have evidence to reject the null hypothesis. We can say that there is a statistically significant association between *Attrition* and J*ob Satisfaction*, *Environment Satisfaction*, *Work-Life Balance*, and *Job Involvement.\

In light of these results, we now want to examine how the overall view of satisfaction changes when considering the average solely between *JobSatisfaction* and *EnvironmentSatisfaction,* expecting an even clearer and more distinct interpretation than before.

    ```{r echo=FALSE, fig.height=3, message=FALSE}

    HR_Analytics$AverageSatisfaction <- (HR_Analytics$JobSatisfaction + HR_Analytics$EnvironmentSatisfaction) / 2

    ggplot(HR_Analytics, aes(y = Attrition, x = AverageSatisfaction, fill=Attrition)) +
      geom_boxplot() +
      scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
      scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
      theme_minimal()
    ```

3.  **Does the decision to work overtime have an impact on the choice to leave the company?**

As we've observed from the overall view of Attrition compared to each variable, it's evident that employees who work overtime exhibit a significantly higher Attrition rate, which stands at 28%. Furthermore, we can see that more than 50% of employees who leave are those who work overtime.

```{r echo=FALSE, fig.height=3, message=FALSE}
ggarrange(
    ggplot(HR_Analytics) + geom_bar(aes(x = OverTime, fill=Attrition), position = 'fill', col='black') +
      scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
      theme_minimal()+
      theme(axis.text.x = element_text(angle = 0, hjust = 1)),
    ggplot(HR_Analytics) + geom_bar(aes(x = Attrition, fill=OverTime), position = 'fill', col='black') +
      theme_minimal()+
      scale_fill_manual(values=c(palette[3],palette[9])) +
      theme(axis.text.x = element_text(angle = 0, hjust = 1)),
    ncol=2,
    nrow=1,
    common.legend = FALSE
)
```

Now, we want to conduct a chi-square test to analyze the relationship between these two variables.
    
    
    ```{r, echo=FALSE}

    chi_square_result <- chisq.test(table(HR_Analytics$Attrition, HR_Analytics$OverTime))

    print(chi_square_result)
    ```

Due to the very small value of p-value we can reject the null hypothesis and statistically confirm that \*OverTime\* and \*Attrition\* are strongly associated.

4.  **How does Monthly Income impact the choice to leave?**

Considering the previous plots that depict the distribution of Monthly Income in relation to Attrition, it becomes apparent that individuals who decide to leave typically earn less.

```{r, echo=FALSE, fig.height=6, fig.width=22, message=FALSE, fig.align='left'}
ggarrange(
      
  ggplot(HR_Analytics) + geom_density(aes(x = MonthlyIncome, fill=Attrition), alpha=0.8) +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
      
  ggplot(HR_Analytics, aes(x = MonthlyIncome, fill=Attrition)) + 
    stat_boxplot(geom = "errorbar", width = 0.75, color = 'black') + 
    geom_boxplot(color='black') +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal(),
  nrow=1,
  ncol=2,
  common.legend = TRUE
)
```

We intend to confirm this by conducting a Wilcoxon rank sum test to compare the distribution of Monthly Income (a numerical variable) between the two distinct groups defined by Attrition.

```{r, echo=FALSE}
wilcox.test(MonthlyIncome ~ Attrition, data = HR_Analytics, exact = FALSE)
```

Due to the small p-value that we have obtained we can reject the null hypotehsis indicating that there is strong evidence of a significant difference in monthly income between employees who have experienced attrition and those who have not.

5.  **Which roles are most critical?**

Having examined potential factors that could influence the decision to leave, our next step is to explore these factors across different *Job Role* to identify areas where intervention may be necessary.

To begin, we'll illustrate how Attrition is distributed among various job roles.
```{r echo=FALSE, message=FALSE, fig.height=3}
ggplot(HR_Analytics) + geom_bar(aes(x = JobRole, fill=Attrition), position = 'fill') +
  scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

It seems that the most critical roles are ***Sales Representative*** (with approximately 40% of Attrition), *Human Resources* and *Laboratory Technician*.

Now, we want to visualize whether some of these roles are more significantly affected by the "younger employee attrition" phenomenon.

```{r echo=FALSE, fig.height=3}
ggplot(HR_Analytics, aes(x = JobRole,
               y = Age,
               group = Attrition,
               fill = Attrition)) + 
  stat_summary(fun = "mean", geom = "bar", position = position_dodge(width = 0.9)) +
  labs(x="JobRole", y="Mean of Age per JobRole") +
  scale_fill_manual(values=custom_colorsD)+
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

We observe that younger employees who choose to leave are predominantly found among the roles of ***Sales Representative***, *Human Resources*, *Laboratory Technician*.

```{r  echo=FALSE, fig.height=5}
ggplot(HR_Analytics, aes(y = JobRole, x = MonthlyIncome, fill = Attrition)) + 
  geom_boxplot() +
  labs() +
  scale_fill_manual(values = custom_colorsD) +
  theme(axis.text.y = element_text(angle = 0, hjust = 0.9))
```

We can notice that the most critical salary-related issues manifest in the following roles: ***Sales Representative***, *Research Scientist*, *Laboratory Technician*, and *Human Resources*. In contrast, an opposite trend is evident in the roles of *Research Director* and *Healthcare Representative*.

We want to determine if there are roles in which the level of satisfaction, as measured by the mean of Job Satisfaction and Environment Satisfaction, is lower compared to others.

```{r  echo=FALSE, fig.height=5}
HR_Analytics$AverageSatisfaction <- (HR_Analytics$JobSatisfaction + HR_Analytics$EnvironmentSatisfaction) / 2

ggplot(HR_Analytics, aes(y = JobRole, x = AverageSatisfaction, fill=Attrition)) +
    geom_boxplot() +
    scale_fill_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +  
    scale_colour_manual(values=c(custom_colorsD[1],custom_colorsD[2])) +
    theme_minimal()
```

With the exception of the Research Director, it appears that those who leave the company tend to have lower overall satisfaction. Notably, this trend is particularly pronounced in roles such as Human Resources, Healthcare Representative, and Sales Representative.

The following plot illustrates that employees who work overtime and decide to leave are predominantly found among Sales Representatives, Laboratory Technicians, and Human Resources.

```{r echo=FALSE, fig.height=5, message=FALSE}
percentage_data <- HR_Analytics %>%
  group_by(JobRole, OverTime) %>%
  summarize(Attrition_Percentage = mean(Attrition == "Yes") * 100)

ggplot(HR_Analytics, aes(JobRole, OverTime)) + 
  geom_jitter(aes(color = Attrition), size = 2, alpha=0.6) +
  scale_fill_manual(values=custom_colorsD) +
  scale_colour_manual(values=custom_colorsD) +
  theme_minimal() +
  geom_text(data = percentage_data,
            aes(label = paste0(round(Attrition_Percentage, 1), "%")),
            size = 4, color = "#52557b", fontface = "bold")+
  theme(axis.text.x = element_text(angle = 45, hjust = 0.9))
```




# Classification
```{r warning=FALSE, include=FALSE}
HR_Analytics <- read.csv("~/Downloads/HR_Analytics.csv")
attach(HR_Analytics)
HR_Analytics <- HR_Analytics[, -c(4, 13)]
HR_Analytics <- HR_Analytics[NumCompaniesWorked!= 0,]
HR_Analytics <- HR_Analytics[, -c(8, 9, 18, 25, 20)]
```
Our intent is to predict whether an employee leaves or not the workplace. Therefore, we decided to use the following classification models:

-   Logistic Regression
-   Logistic Regression with Ridge Regularization
-   Logistic Regression with Lasso Regularization
-   Linear Discriminant Analysis
-   Quadratic Discriminant Analysis
-   Naive Bayes
-   KNN

Notice that it is necessary to encode categorical variables so that they can be considered numeric and hence be used in the models: we did so by converting to numeric the levels of such columns.

```{r include=FALSE}
HR_Analytics$MaritalStatus <- factor(HR_Analytics$MaritalStatus, 
                                     levels = c("Single", "Married", "Divorced"))
HR_Analytics$MaritalStatus <- as.numeric(HR_Analytics$MaritalStatus) - 1

HR_Analytics$BusinessTravel <- factor(HR_Analytics$BusinessTravel, 
                                      levels = c("Non-Travel", "Travel_Rarely", "Travel_Frequently"))
HR_Analytics$BusinessTravel <- as.numeric(HR_Analytics$BusinessTravel) - 1

HR_Analytics$Department <- factor(HR_Analytics$Department, levels = c("Sales", "Research & Development", "Human Resources" ))
HR_Analytics$Department <- as.numeric(HR_Analytics$Department) - 1

HR_Analytics$EducationField <- factor(HR_Analytics$EducationField, levels = c("Life Sciences","Other","Medical","Marketing","Technical Degree","Human Resources"))
HR_Analytics$EducationField <- as.numeric(HR_Analytics$EducationField) - 1

HR_Analytics$Gender <- factor(HR_Analytics$Gender, levels = c("Male","Female" ))
HR_Analytics$Gender <- as.numeric(HR_Analytics$Gender) -1

HR_Analytics$JobRole <- factor(HR_Analytics$JobRole, levels = c("Sales Executive","Research Scientist","Laboratory Technician","Manufacturing Director","Healthcare Representative","Manager","Sales Representative","Research Director","Human Resources"))
HR_Analytics$JobRole <- as.numeric(HR_Analytics$JobRole)-1

HR_Analytics$Attrition <- factor(HR_Analytics$Attrition, levels = c("No","Yes"))
HR_Analytics$Attrition <- as.numeric(HR_Analytics$Attrition) - 1

HR_Analytics$OverTime <- factor(HR_Analytics$OverTime, levels = c("No","Yes" ))
HR_Analytics$OverTime <- as.numeric(HR_Analytics$OverTime) - 1
```

Let us define a function that allows to rapidly and effectively compute metrics such as *accuracy*, *precision*, *recall*, *specificity* and *F1 score*, given the respective confusion matrix.

```{r}
compute_metrics <- function(confusion_matrix) {
  tp <- confusion_matrix[2, 2]
  tn <- confusion_matrix[1, 1]
  fp <- confusion_matrix[2, 1]
  fn <- confusion_matrix[1, 2]
  accuracy <- (tp + tn) / sum(confusion_matrix)
  precision <- tp / (tp + fp)
  recall <- tp / (tp + fn)
  specificity <- tn / (tn + fp)
  f1_score <- 2 * (precision * recall) / (precision + recall)
  metrics = data.frame(metric = c("Accuracy", "Precision", "Recall", "Specificity", "F1 score"),
                       values = c(accuracy, precision, recall, specificity, f1_score))
  print(t(metrics))
}
```

We reckon that our priority is to detect people that intend to leave the workplace, therefore the metric we aim to maximize is *recall*. In fact, we are less concerned to misclassify an employee that doesn't actually plan on leaving the company than missing someone who intents to quit. Also, as we are dealing with an unbalanced dataset, we are going to prioritize the F1-score metric over accuracy, as the former factors in, by definition, both recall and precision, thus taking in consideration the prediction performance of both classes.

Lastly, we split our dataset in training set and test set with a 75/25 ratio, setting a seed for reproducibility purposes.

```{r}
set.seed(108)

n <- nrow(HR_Analytics)
train <- sample(1:n, round(n*0.75))
test <- setdiff(1:n, train)

train.data <- HR_Analytics[train, ]
test.data <- HR_Analytics[test, ]
table(test.data$Attrition)
```

## Logistic Regression with Unbalanced Dataset

The first model used is Logistic Regression.

```{r}
full.model <- glm(train.data$Attrition ~ ., data = train.data, family = binomial)
summary(full.model)
```

We can test different thresholds, minding that theoretically a smaller threshold is fitted to improve *recall*. We are trying out 0.4, 0.5 and 0.6.

```{r}

predicted <- predict(full.model, newdata = test.data, type = "response")

for (t in c(0.4, 0.5, 0.6)){
predicted.classes <- ifelse(predicted > t, 1, 0)
confusion.matrix <- table(predicted.classes, test.data$Attrition)
print(confusion.matrix)
compute_metrics(confusion.matrix)
}
```
As we could have foreseen, a threshold of 0.4 best fits our purpose. This way, our model will tend to easily classify more instances as 1. Nevertheless, we think that such results can be further improved.

To assess collinearity in the model, we think it is advisable to compute the Variance Inflation Factor (VIF), reputing any variable with VIF over 5 problematic. In fact, a high VIF for a variable indicates that it is strongly correlated with other independent variables in the model and this can lead to unstable coefficient estimates and decreased interpretability.

```{r}
vif(full.model)
```

It's clear that there are only a few troublesome collinear variables having VIF values over 5. We have procede to eliminate them one by one from our model, ultimately producing the following model where the variables *JobLevel* and *YearsAtCompany* are gone.

```{r}
full.model.1 <- glm(train.data$Attrition ~ . - JobLevel - YearsAtCompany, data = train.data, family = binomial)
summary(full.model.1)
```


We can notice from the summary that there are still a lot of non significant variables (with a high p-value) in this model, but we can further improve this using a feature selection approach: we tested both *forward* and *backward* algorithms, using the function *step*.

In the forward feature selection algorithm, we start with a model with only one variable and progressively add one variable at a time so that the model performance is improved. On the other hand, the backward algorithm consists of iteratively removing one variable at a time starting from the complete model in such a way that the AIC keeps decreasing. AIC stands for Akaike Information Criterion and statistically quantifies the compromise between a good fitting model and its complexity.

We have observed that the backward selection algorithm provides a more performing model with a lower AIC, so that is the algorithm that we are going to keep using for the entire project.


```{r include=FALSE}
full.model.step <- step(full.model.1, direction = "backward")
```

```{r}
summary(full.model.step)
```
The function *step* has produced a model in which variables *Education*, *PerformanceRating* and *StockOptionLevel* are excluded, whereas the most significant features are:

-   *BusinessTravel*
-   *EnvironmentSatisfaction*
-   *JobInvolvment*
-   *MaritalStatus*
-   *NumCompaniesWorked*
-   *OverTime*
-   *WorkLifeBalance*
-   *YearsSinceLastPromotion*

In particular, *OverTime* has a significantly smaller p-value (<2e-16) compared to the other predictor variables.

From now on, for every model we are still going to try out different thresholds (choosing from 0.4, 0.5, 0.6), but for brevity purposes we are going to print directly the best model out of the three.
Here follows the confusion matrix and the relative metrics obtained by the model, using a threshold of 0.4:

```{r echo=FALSE}

predicted.step <- predict(full.model.step, newdata = test.data, type = "response")
predicted.classes.step <- ifelse(predicted.step > 0.4, 1, 0)
confusion.matrix.step <- table(predicted.classes.step, test.data$Attrition)
print(confusion.matrix.step)
#compute_metrics(confusion.matrix.step)
metrics_lr_unb=compute_metrics(confusion.matrix.step)
```

```{r echo=TRUE}
df_metrics <- data.frame(Model_name = character(),
                         Accuracy = numeric(),
                         Precision = numeric(),
                         Recall = numeric(),
                         F1_score = numeric(),
                         PR_AUC = numeric())
colnames(df_metrics) <- c("Model_name", "Accuracy", "Precision", "Recall", "F1_score", "PR_AUC")

Model_name <- "Logistic Regression Unbalanced"


accuracy <- as.numeric(metrics_lr_unb[2, which(metrics_lr_unb[1, ] == "Accuracy")])
precision <- as.numeric(metrics_lr_unb[2, which(metrics_lr_unb[1, ] == "Precision")])
recall <- as.numeric(metrics_lr_unb[2, which(metrics_lr_unb[1, ] == "Recall")])
specificity <- as.numeric(metrics_lr_unb[2, which(metrics_lr_unb[1, ] == "Specificity")])
f1_score <- as.numeric(metrics_lr_unb[2, which(metrics_lr_unb[1, ] == "F1 score")])

class0_indices = which(test.data$Attrition == 0)
pr_curve_glm_unb <- pr.curve(scores.class0 = predicted.classes.step[class0_indices], scores.class1 = predicted.classes.step[-class0_indices], curve=TRUE, max.compute = T, min.compute = T, rand.compute = T)
pr_auc_unb <- as.numeric(pr_curve_glm_unb$auc.integral)

new_row <- data.frame(Model_name = Model_name,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      PR_AUC = pr_auc_unb)

df_metrics <- rbind(df_metrics, new_row)
```
## Logistic Regression with Balanced Dataset

Since we are working with an unbalanced dataset, it's recommended to employ sampling techniques to address the class imbalance issue. This approach will enable the model to capture patterns from the minority class, which is crucial, as our main objective is to accurately detect individuals who intend to leave. We'll focus solely on balancing the training set using the *ovun.sample* function and the *SMOTE* method while maintaining the test set's original ratio. Furthermore, we continue to take into account the considerations derived from the VIF values of the full model.

Keep in mind that we are going to thoroughly address every sampling technique specifically for logistic regression. In fact, we have come to the conclusion that oversampling produces the most performant models. Therefore, we will only focus on unbalanced and oversampled training sets for the rest of the models.

### Sampling with ovun.sample

We initially opted for undersampling our data using the ovun.sample function. However, we eventually decided to discard this approach because it resulted in a training set even smaller than the test set. In the end, we chose to sample the training set while maintaining its original size, ensuring that the two classes are perfectly balanced with a probability of p = 0.5.

```{r}
under.train.data <- ovun.sample(Attrition ~ ., data = train.data, N = nrow(train.data), p = 0.5, seed=108)$data
```

The training data is now split in the following way:

```{r}
table(under.train.data$Attrition)
```

Here follows the application of the Logistic Regression model on the new training set.

```{r}
full.model.under <- glm(under.train.data$Attrition ~ . - JobLevel - YearsAtCompany, data = under.train.data, family = binomial)
summary(full.model.under)
```

Notice that this model has a higher AIC than the previous models. The features with 0 significance level are the following:

-   *BusinessTravel*

-   *EnvironmentSatisfaction*

-   *JobInvolvement*

-   *JobSatisfation*

-   *MaritalStatus*

-   *NumCompaniesWorked*

-   *OverTime*

-   *WorkLifeBalance*

-   *YearsInCurrentRole*

-   *YearsSinceLastPromotion*


```{r echo=FALSE}
predicted.under <- predict(full.model.under, newdata = test.data, type = "response")
predicted.classes.under <- ifelse(predicted.under > 0.4, 1, 0)
confusion_matrix.u <- table(predicted.classes.under, test.data$Attrition)
print(confusion_matrix.u)
compute_metrics(confusion_matrix.u)
```

Evidently, the chosen threshold of 0.4 provides a reasonable compromise between recall and F1-score. According to this model, the two most significant features are *OverTime* and *Department*.

As previously, we will select some features, and as expected, a backward approach is preferred.

```{r include=FALSE}
model.step.under <- step(full.model.under, direction = "backward")
```

```{r}
summary(model.step.under)
```

We can see that the function *step* has removed in the model *model.step.under* the features:

-   *Age*

-   *JobRole*

-   *StockOptionLevel*

-   *YearsWithCurrentManager*

It yields the following results using a threshold of 0.4:

```{r echo=FALSE}
predicted.step.under <- predict(model.step.under, newdata = test.data, type = "response")
predicted.classes.step.under <- ifelse(predicted.step.under > 0.4, 1, 0)
confusion.matrix.step.under <- table(predicted.classes.step.under, test.data$Attrition)
```

```{r}
print(confusion.matrix.step.under)
compute_metrics(confusion.matrix.step.under)
```

```{r}
class0_indices = which(test.data$Attrition == 0)
pr_curve_glm <- pr.curve(scores.class0 = predicted.step.under[class0_indices], scores.class1 = predicted.step.under[-class0_indices], curve=TRUE, max.compute = T, min.compute = T, rand.compute = T)
plot(pr_curve_glm, max.plot = FALSE, min.plot = FALSE, rand.plot = FALSE, fill.area = FALSE, color = 2, auc.main = FALSE, ylim = c(0, 1))
lr_auc<- as.numeric(pr_curve_glm$auc.integral)

cat("Area Under the Curve (AUC):", lr_auc, "\n")
text(0.6, 0.4, paste("AUC =", round(lr_auc, 4)), col = "black", cex = 1.2)
```


### Oversampling

We continued to use the ovun.sample method to oversample the training set. This method generates new instances of the minority class until the class distribution becomes balanced.

```{r}
over.train.data <- ovun.sample(Attrition ~ ., 
                               data = train.data, p = 0.5, method = "over", seed=108)$data
table(over.train.data$Attrition)

```

The Logistic Regression model trained on the oversampled training set is the following:

```{r include=FALSE}
full.model.over <- glm(over.train.data$Attrition ~ . - JobLevel - YearsAtCompany, data = over.train.data, family = binomial)
```

```{r}
summary(full.model.over)
```

We can notice that the only variables with a p-value larger than 0.1 are *PerformanceRating*, *StockOptionLevel* and *TotalWorkingYears*.

These are the results:

```{r echo=FALSE}
predicted.over <- predict(full.model.over, newdata = test.data, type = "response")
predicted.classes.over <- ifelse(predicted.over > 0.45, 1, 0)
confusion_matrix.o <- table(predicted.classes.over, test.data$Attrition)
print(confusion_matrix.o)
#compute_metrics(confusion_matrix.o)
metrics_lr <- compute_metrics(confusion_matrix.o)
```

```{r, echo=FALSE, message=FALSE}
pr_curve_glm_over <- pr.curve(scores.class0= predicted.over[class0_indices], scores.class1 = predicted.over[-class0_indices], curve=TRUE, max.compute = T, min.compute = T, rand.compute = T)
plot(pr_curve_glm_over, max.plot = FALSE, min.plot = FALSE, rand.plot = FALSE, fill.area = FALSE, color = 2, auc.main = FALSE, ylim = c(0, 1))
lr_auc_over<- as.numeric(pr_curve_glm_over$auc.integral)
text(0.6, 0.4, paste("AUC =", round(lr_auc, 4)), col = "black", cex = 1.2)
```

```{r message=FALSE, include=FALSE}
Model_name <- "Logistic Regression "

predicted.classes.over <- ifelse(predicted.over > 0.45, 1, 0)
confusion_matrix.o <- table(predicted.classes.over, test.data$Attrition)

metrics_lr <- compute_metrics(confusion_matrix.o)

accuracy <- as.numeric(metrics_lr[2, which(metrics_lr[1, ] == "Accuracy")])
precision <- as.numeric(metrics_lr[2, which(metrics_lr[1, ] == "Precision")])
recall <- as.numeric(metrics_lr[2, which(metrics_lr[1, ] == "Recall")])
specificity <- as.numeric(metrics_lr[2, which(metrics_lr[1, ] == "Specificity")])
f1_score <- as.numeric(metrics_lr[2, which(metrics_lr[1, ] == "F1 score")])

class0_indices = which(test.data$Attrition == 0)
pr_curve_glm <- pr.curve(scores.class0 = predicted.over[class0_indices], scores.class1 = predicted.over[-class0_indices], curve=TRUE, max.compute = T, min.compute = T, rand.compute = T)
pr_auc <- as.numeric(pr_curve_glm$auc.integral)

new_row <- data.frame(Model_name = Model_name,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      PR_AUC = pr_auc)

df_metrics <- rbind(df_metrics, new_row)

```


The threshold that maximizes recall is 0.4, which results in only 12 leaving employees being misclassified, with minimal impact on the F1 score. We will proceed with the backward selection using this threshold.

```{r include=FALSE}
model.step.over <- step(full.model.over, direction = "backward")
```

```{r}
summary(model.step.over)
```

The algorithm removed exactly the columns *PerformanceRating*, *StockOptionLevel* and *TotalWorkingYears* that were just mentioned because of their high p-value. 

```{r include=FALSE}
predicted.step.over<- predict(model.step.over, newdata = test.data, type = "response")
predicted.classes.step.over <- ifelse(predicted.step.over > 0.45, 1, 0)
confusion.matrix.step.over <- table(predicted.classes.step.over, test.data$Attrition)
```

```{r echo=FALSE}
print(confusion.matrix.step.over)
metrics_lr.step<-compute_metrics(confusion.matrix.step.over)
```

```{r message=FALSE, include=FALSE}
Model_name <- "Logistic regression with Backward Selection"

accuracy <- as.numeric(metrics_lr.step[2, which(metrics_lr.step[1, ] == "Accuracy")])
precision <- as.numeric(metrics_lr.step[2, which(metrics_lr.step[1, ] == "Precision")])
recall <- as.numeric(metrics_lr.step[2, which(metrics_lr.step[1, ] == "Recall")])
specificity <- as.numeric(metrics_lr.step[2, which(metrics_lr.step[1, ] == "Specificity")])
f1_score <- as.numeric(metrics_lr.step[2, which(metrics_lr.step[1, ] == "F1 score")])

pr_curve_glm_step <- pr.curve(scores.class0 = predicted.step.over[class0_indices], scores.class1 = predicted.step.over[-class0_indices], curve=TRUE, max.compute = T, min.compute = T, rand.compute = T)
pr_auc.step <- as.numeric(pr_curve_glm_step$auc.integral)

new_row <- data.frame(Model_name = Model_name,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      PR_AUC = pr_auc.step)

df_metrics <- rbind(df_metrics, new_row)
```
### SMOTE

SMOTE (Synthetic Minority Over-sampling Technique) is a data augmentation that generates synthetic examples for the minority class.

```{r}
train.data$Attrition = as.factor(train.data$Attrition)
smote.train.data <- smote(Attrition ~ ., train.data, perc.over = 2, k = 5)
table(smote.train.data$Attrition)
```

The model trained on the SMOTEd training set is:

```{r}
full.model.smote <- glm(smote.train.data$Attrition ~ . - JobLevel - YearsAtCompany, data = smote.train.data, family = binomial)
```

```{r}
summary(full.model.smote)
```

As it is evident from the summary, there are a lot of significant features that yield this result:

```{r}
predicted.smote <- predict(full.model.smote, newdata = test.data, type = "response")
predicted.classes.smote <- ifelse(predicted.smote > 0.4, 1, 0)
confusion_matrix.s <- table(predicted.classes.smote, test.data$Attrition)
print(confusion_matrix.s)
compute_metrics(confusion_matrix.s)
```

The model with threshold 0.4 reaches a 75% recall with a 0.48% F1-score. The backward algorithm is able to reduce the AIC from 1444 to 1023, removing the columns *StockOptionLevel*, *PerformanceRatings*, *TotalWorkingYears*, *TrainingTimesLastYear*, *YearsInCurrentRole*. However, the performance ever so slighly decreases so we think it's not worth mentioning it. 


## Shrinkage Methods

Instead of managing model complexity through subset selection methods, we can fit a model with all the predictors using techniques that constrain the coefficient estimates. In our case, we've chosen to apply Ridge and Lasso regression. Ridge regression uses quadratic shrinking, while Lasso regression uses absolute-value shrinking.

### Ridge Regularization

Ridge regularization, also known as L2 regularization, adds a penalty term to the loss function, which corresponds to the sum of the squared values of the model's coefficients. This penalty term is essentially the L2 norm of the coefficients and is represented as:

$\lambda\sum_{j=1}^{p}\beta_j^2$

This regularization method encourages the model to keep the coefficients of the variables small, pushing them towards zero.

The shrinkage term is controlled by the hyperparameter $\lambda$, which we will select using cross-validation. We will consider a range of possible values for $\lambda$ from 100,000 to 0.001.


```{r echo=TRUE}
grid <- 10^seq(5, -3, length=100)
```

#### Ridge Regularization with Unbalanced Dataset

We define the model *ridge.mod* specifying the grid of lambda values for cross-validation.

```{r echo=TRUE}
X <- model.matrix(Attrition ~ ., data = HR_Analytics)
X <- X[,-1]
y <- HR_Analytics$Attrition
y.test <- y[test]
ridge.mod <- cv.glmnet(X[train,], y[train], alpha = 0,
                    lambda = grid, thresh = 1e-12, standardize = TRUE, nfold = 10)
bestlam <- ridge.mod$lambda.min
```

```{r echo=FALSE, warning=FALSE, fig.align='left'}
plot(ridge.mod)
```
These are the results of unbalanced Ridge Regression using the optimal lambda value:
```{r}
bestlam
```

```{r echo=TRUE}
ridge.pred <- predict(ridge.mod, s = bestlam, newx = X[test, ])
ridge.pred_class <- ifelse(ridge.pred > 0.3, 1, 0)
ridge_confusion_mat <- table(ridge.pred_class, y[test])
print(ridge_confusion_mat)
ridge.metrics <- compute_metrics(ridge_confusion_mat)
```

From the plot and the coefficients printed below, it is evident that there is a particularly significant feature: *OverTime*. Other important variables are *BusinessTravel*, *MaritalStatus* , *WorkLifeBalance* and *EnvironmentSatisfaction*.

```{r echo=TRUE}
ridge.mod_un <- glmnet(X, y, alpha=0, lambda = grid, thresh = 1e-12, , standardize = TRUE)
plot(ridge.mod_un, xvar="lambda", label=TRUE)
```
```{r echo=TRUE}
selected_coeffs <- coefficients(ridge.mod_un, s = bestlam)
selected_coeffs
```

#### Ridge Regularization with Oversampling

```{r echo=TRUE}
X1 <- model.matrix(Attrition ~ . , data = over.train.data)
X1 <- X1[,-1]
y <- over.train.data$Attrition
y <- as.numeric(over.train.data$Attrition)
X2 <- model.matrix(Attrition~ . , data = test.data)
X2 <- X2[,-1]
y.test <- test.data$Attrition
```

```{r echo=TRUE}
plot(ridge.mod)
```

```{r echo=FALSE}
ridge.mod.over <- cv.glmnet(X1, y, alpha=0, lambda=grid, thresh = 1e-12, nfold = 10, standardize = TRUE)
bestlam.over <- ridge.mod.over$lambda.min
bestlam.over 
ridge.pred.over <- predict(ridge.mod.over, s = bestlam.over, newx = X2)
ridge.pred_class.over <- ifelse(ridge.pred.over > 0.45, 1, 0)
ridge_confusion_mat.over <- table(ridge.pred_class.over, test.data$Attrition)
ridge_confusion_mat.over
metrics.ridge.over <- compute_metrics(ridge_confusion_mat.over)
```

By looking at the plot below, it is obvious that there are two features that strongly dominate over the others (which may justify the substantial improvement from the unbalanced Ridge model): 15 and 2, being respectively *OverTime* and *BusinessTravel*, followed by features like *MaritalStatus*, *Gender* , *WorkLifeBalance* and *Department*.

```{r echo=TRUE}
ridge.mod.ov <- glmnet(X1, y, alpha=0, lambda = grid, thresh = 1e-12, , standardize = TRUE)
plot(ridge.mod.ov, xvar="lambda", label=TRUE)
```

```{r echo=TRUE}
selected_coeffs <- coefficients(ridge.mod.ov, s = bestlam)
selected_coeffs
```

```{r echo=FALSE}
pr_curve_ridge_over <- pr.curve(scores.class0 = ridge.pred.over[class0_indices], scores.class1 = ridge.pred.over[-class0_indices], curve=TRUE, max.compute = T, min.compute = T, rand.compute = T)
plot(pr_curve_ridge_over, max.plot = FALSE, min.plot = FALSE, rand.plot = FALSE, fill.area = FALSE, color = 2, auc.main = FALSE, ylim = c(0, 1))
lr_auc_ridge<- as.numeric(pr_curve_ridge_over$auc.integral)

cat("Area Under the Curve (AUC):", lr_auc_ridge, "\n")
text(0.6, 0.4, paste("AUC =", round(lr_auc_ridge, 4)), col = "black", cex = 1.2)
```


```{r message=FALSE, include=FALSE}
Model_name <- "Ridge Regularization"
accuracy <- as.numeric(metrics.ridge.over[2, which(metrics.ridge.over[1, ] == "Accuracy")])
precision <- as.numeric(metrics.ridge.over[2, which(metrics.ridge.over[1, ] == "Precision")])
recall <- as.numeric(metrics.ridge.over[2, which(metrics.ridge.over[1, ] == "Recall")])
specificity <- as.numeric(metrics.ridge.over[2, which(metrics.ridge.over[1, ] == "Specificity")])
f1_score <- as.numeric(metrics.ridge.over[2, which(metrics.ridge.over[1, ] == "F1 score")])

pr_curve_glm_ridge <- pr.curve(scores.class0 = ridge.pred.over[class0_indices], scores.class1 = ridge.pred.over[-class0_indices], curve=TRUE, max.compute = T, min.compute = T, rand.compute = T)
pr_auc.ridge <- as.numeric(pr_curve_glm_ridge$auc.integral)

new_row <- data.frame(Model_name = Model_name,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      PR_AUC = pr_auc.ridge)

df_metrics <- rbind(df_metrics, new_row)
```

### Lasso Regularization

Lasso regularization, also known as L1 regularization, adds a penalty term that consists of the sum of the absolute values of the model's coefficients:

$\lambda\sum_{j=1}^{p}|\beta_j|$

This encourages the model's coefficients to become smaller, and some of them may become exactly zero. In fact, Lasso is a sparsity-inducing approach where the nonzero coefficients left are associated with significant variables.

Similar to Ridge regularization, the penalty term is controlled by a hyperparameter $\lambda$, which is selected through cross-validation. The search for the optimal $\lambda$ involves exploring a grid of possible values ranging from 1000 to 0.0001

```{r echo=TRUE}
grid <- 10^seq(3, -4, length=100)
```

#### Lasso Regularization with Unbalanced Dataset

Let's define the model *lasso.mod*, providing the grid of values from which the function *cv.glmnet* can peform cross-validation. It will allow to retrieve the $\lambda$ that yields the best result.

```{r echo=TRUE}
X <- model.matrix(Attrition ~ . , data = HR_Analytics)
X <- X[,-1]
y <- HR_Analytics$Attrition
y.test <- y[test]
lasso.mod <- cv.glmnet(X[train, ], y[train], alpha = 1,
                    lambda = grid, thresh = 1e-12, standardize = TRUE, nfold = 10)
bestlam <- lasso.mod$lambda.min
```

```{r echo=TRUE, warning=FALSE, fig.align='left'}
plot(lasso.mod)
```

We can now let our model predict using the optimal $\lambda$ obtained and a threshold of 0.4.
```{r}
lasso.predict <- predict(lasso.mod, s = bestlam, newx = X[test, ])
lasso.predict_class <- ifelse(lasso.predict > 0.3, 1, 0)
lasso_confusion_mat <- table(lasso.predict_class, y[test])
lasso_confusion_mat
lasso.metrics <- compute_metrics(lasso_confusion_mat)
```

The plot gives a clear representation of how the variable *Overtime*, in this model as well, proves to be the most meaningful.
Other noteworthy predictors are *BusinessTravel*, *Department* and *JobInvolvment*.

```{r echo=TRUE}
ridge.mod2 <- glmnet(X, y, alpha=1, lambda = grid, thresh = 1e-12, , standardize = TRUE)
plot(ridge.mod2, xvar="lambda", label=TRUE)
```

```{r echo=TRUE}
selected_coeffs <- coefficients(lasso.mod, s = bestlam)
selected_coeffs
```

#### Lasso Regularization with Oversampling

Let's see how Lasso Regularization performs using the oversampled training set.

As before, thanks to cross validation, we can retrive the best lambda:
```{r echo=TRUE, fig.align='left'}
X1 <- model.matrix(Attrition ~ . - JobLevel - YearsAtCompany, data = over.train.data)
X1 <- X1[,-1]
y <- over.train.data$Attrition
y <- as.numeric(over.train.data$Attrition)
X2 <-  model.matrix(Attrition ~ . - JobLevel - YearsAtCompany, data = test.data)
X2 <- X2[,-1]
y.test <- test.data$Attrition
lasso.mod <- cv.glmnet(X1, y, alpha=1, lambda=grid, standardized=TRUE, nfold = 10)
plot(lasso.mod, label=TRUE)
bestlam.lasso.over <- lasso.mod$lambda.min
```

```{r echo=FALSE}
print(bestlam.lasso.over)
```

```{r echo=TRUE}
lasso.pred.over <- predict(lasso.mod, s=bestlam.lasso.over, newx=X2, type="response")
lasso.over <- ifelse(lasso.pred.over > 0.44, 1, 0)
lasso_confusion_mat.over <- table(lasso.over, test.data$Attrition)
lasso_confusion_mat.over
metrics.lasso <- compute_metrics(lasso_confusion_mat.over)
```

As you can notice from the plot and the coefficients below, the variable *OverTime* once again contributes the most to the prediction.

```{r echo=TRUE, fig.align='left'}
ridge.mod2 <- glmnet(X1, y, alpha=1, lambda = grid, thresh = 1e-12, , standardize = TRUE)
plot(ridge.mod2, xvar="lambda", label=TRUE)
```

```{r echo=TRUE}
selected_coeffs <- coefficients(lasso.mod, s = bestlam)
selected_coeffs
```

```{r, echo=FALSE, message=FALSE}
Model_name <- "Lasso Regularization"

accuracy <- as.numeric(metrics.lasso[2, which(metrics.lasso[1, ] == "Accuracy")])
precision <- as.numeric(metrics.lasso[2, which(metrics.lasso[1, ] == "Precision")])
recall <- as.numeric(metrics.lasso[2, which(metrics.lasso[1, ] == "Recall")])
specificity <- as.numeric(metrics.lasso[2, which(metrics.lasso[1, ] == "Specificity")])
f1_score <- as.numeric(metrics.lasso[2, which(metrics.lasso[1, ] == "F1 score")])

pr_curve_glm_lasso<- pr.curve(scores.class0 = lasso.pred.over[class0_indices], scores.class1 = lasso.pred.over[-class0_indices], curve=TRUE, max.compute = T, min.compute = T, rand.compute = T)
pr_auc.lasso <- as.numeric(pr_curve_glm_lasso$auc.integral)

new_row <- data.frame(Model_name = Model_name,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      PR_AUC = pr_auc.lasso)

df_metrics <- rbind(df_metrics, new_row)
```


## Naive Bayes

The Naive Bayes model is a classification model that is based on Bayes' theorem. It assumes a strong independence among predictors, hence the 'naive' designation. However, it's important to note that this independence condition is not satisfied, as observed in the exploratory data analysis (EDA), but we proceed with the model anyway.

#### Naive Bayes with Unbalanced Data

Here follows the Naive Bayes model applied to the unbalanced training set, using a threshold of 0.4.

```{r echo=TRUE, warning=FALSE}

train.data$Attrition <- as.factor(train.data$Attrition)

naivebayes.model <- naive_bayes(Attrition ~ . - JobLevel - YearsAtCompany, data = train.data)
naivebayes.prediction <- predict(naivebayes.model, newdata = test.data, type = "prob")
naivebayes.y_pred_class <- ifelse(naivebayes.prediction[, "1"] > 0.4, 1, 0)
naivebayesmat <- table(naivebayes.y_pred_class, test.data$Attrition)
naivebayesmat
naivebayes.metrics <- compute_metrics(naivebayesmat)
```

#### Naive Bayes with Oversampling

Using the oversampled training set and maintaining the same threshold, the recall improves by circa 10% percent. Of course, such tendency clearly penalizes the overall F1-score that drops by a lot.

```{r echo=FALSE, warning=FALSE}
over.train.data$Attrition <- as.factor(over.train.data$Attrition)

naivebayes.model.over <- naive_bayes(Attrition ~ ., data = over.train.data)
naivebayes.prediction.over <- predict(naivebayes.model.over, newdata = test.data, type = "prob")
naivebayes.pred_over <- ifelse(naivebayes.prediction.over[, "1"] > 0.4, 1, 0)
naivebayes.conf.mat.over <- table(naivebayes.pred_over, test.data$Attrition)
print(naivebayes.conf.mat.over)
metrics.nb.over <- compute_metrics(naivebayes.conf.mat.over)
```


```{r, echo=FALSE, message=FALSE}
Model_name <- "Naive Bayes"

accuracy <- as.numeric(metrics.nb.over[2, which(metrics.nb.over[1, ] == "Accuracy")])
precision <- as.numeric(metrics.nb.over[2, which(metrics.nb.over[1, ] == "Precision")])
recall <- as.numeric(metrics.nb.over[2, which(metrics.nb.over[1, ] == "Recall")])
specificity <- as.numeric(metrics.nb.over[2, which(metrics.nb.over[1, ] == "Specificity")])
f1_score <- as.numeric(metrics.nb.over[2, which(metrics.nb.over[1, ] == "F1 score")])

pr_curve_glm_nb<- pr.curve(scores.class0 = naivebayes.prediction.over[class0_indices], scores.class1 = naivebayes.prediction.over[-class0_indices], curve=TRUE, max.compute = T, min.compute = T, rand.compute = T)
pr_auc.nb <- as.numeric(pr_curve_glm_nb$auc.integral)

new_row <- data.frame(Model_name = Model_name,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      PR_AUC = pr_auc.nb)

df_metrics <- rbind(df_metrics, new_row)
```

## LDA

Linear Discriminant Analysis is a classification model that also aims at detecting a subspace that maximizes the separation between the classes while minimizing the variation within each class, so that our data lays in a subspace where they are well-separated.

LDA is based on two assumptions:

-   data within each class follows a normal distribution;
-   covariance matrices of different classes are equal.

Let's check if our features are normally distributed by defining the following function that allows us to visibly estimate the normality of our dataset columns by plotting the density and QQ plots. Furthermore, it computes the Shapiro-Wilk test that numerically assesses normality.

```{r}
check.for.normality <- function(column) {
  par(mfrow = c(1,2))
  plot(density(column), main = paste("Density Plot of", colnames(HR_Analytics)[i]), xlab = "...")
  qqnorm(y=HR_Analytics[,i],main= paste("QQ plot of",colnames(HR_Analytics)[i],sep = " "))
  qqline(y=HR_Analytics[,i])
  par(mfrow=c(1,1))
  print(paste("Shapiro-Wilk Test",colnames(HR_Analytics)[i], sep=' '))
  print(shapiro.test(column))
}
```

For brevity, we are only going to show the analysis of two variables: except for the variable *Age*, the Shapiro-Wilk test always results in a p-value smaller than 2.2e-16. We can conclude that our variables don't have a normal distribution, but we are going to proceed with the application of LDA regardless, expecting that the model performance is affected by the fact that the two assumptions are not met.

```{r echo=FALSE}
for(i in c(1,6)){
  check.for.normality(HR_Analytics[,i])
}
```

### LDA with Unbalanced Dataset

Here follows the application of LDA to our unbalanced training set. We are trying out different thresholds to determine which one improves the model performance, choosing from the usual range of values 0.4, 0.5 and 0.6.

```{r}
lda.fit <- lda(Attrition ~ . - JobLevel - YearsAtCompany, data = train.data)
lda.pred <- predict(lda.fit, test.data, type="response")
lda.res <- lda.pred$posterior
for (t in c(0.4, 0.5, 0.6)){
lda.pred.best <- as.factor(ifelse(lda.res[,2] > t, 1, 0))
lda.conf.mat <- t(table(test.data$Attrition, lda.pred.best))
print(lda.conf.mat)
compute_metrics(lda.conf.mat)
lda.error <-  mean(test.data$Attrition != lda.pred.best)
print(lda.error)
}
```

Evidently, the threshold 0.6 induces a significantly higher recall if compared to the other two models, without penalizing excessively the F1-score. Such model also provides the lower mean error among the thresholds, so it's easy to conclude that the model with threshold 0.4 has the best performance.

To keep it brief, we are reporting below only a few coefficients to prove that the main direction - the one with the highest magnitude - is *OverTime*. It is the variable that best separates the data, even though it is clear that it doesn't do it sufficiently, just like we were expecting given the non linear separability of our data. Such property can be seen thanks to LDA: the density of the two classes are largely overlapping.

```{r echo=FALSE}
main.dimension <- lda.fit$scaling
for (i in 14:17) {
  print(paste0(rownames(main.dimension)[i], " ",  main.dimension[i]))
}
```

```{r echo=FALSE}
plot(lda.fit)
```

```{r echo=FALSE}
lda_res <- lda(Attrition~., data=HR_Analytics, center = TRUE, scale = TRUE)
lda_df <- predict(lda_res, HR_Analytics)$x %>% as.data.frame() %>% cbind(Attrition=HR_Analytics$Attrition)
ggplot(lda_df, aes(x=LD1, y=0, col=Attrition)) + geom_point(alpha=0.5)
```

###  LDA with Oversampling

The results derived from the application of LDA model on the oversampled training data show quite a drastic improvement in the model's performance. 

```{r}
lda.fit <- lda(Attrition ~ .  - JobLevel - YearsAtCompany, data = over.train.data)
lda.pred_over <- predict(lda.fit, test.data, type="response")
lda.res_over <- lda.pred_over$posterior

lda.pred.best <- as.factor(ifelse(lda.res_over[,2] > 0.415, 1, 0))
lda.conf.mat <- t(table(test.data$Attrition, lda.pred.best))
print(lda.conf.mat)
metrics.lda=compute_metrics(lda.conf.mat)
lda.error <-  mean(test.data$Attrition != lda.pred.best)
print(lda.error)
```
```{r}
plot(lda.fit)
```

```{r, echo=FALSE, message=FALSE}
Model_name <- "LDA"

accuracy <- as.numeric(metrics.lda[2, which(metrics.lda[1, ] == "Accuracy")])
precision <- as.numeric(metrics.lda[2, which(metrics.lda[1, ] == "Precision")])
recall <- as.numeric(metrics.lda[2, which(metrics.lda[1, ] == "Recall")])
specificity <- as.numeric(metrics.lda[2, which(metrics.lda[1, ] == "Specificity")])
f1_score <- as.numeric(metrics.lda[2, which(metrics.lda[1, ] == "F1 score")])

pr_curve_glm_lda<- pr.curve(scores.class0 = lda.pred.best[class0_indices], scores.class1 = lda.pred.best[-class0_indices], curve=TRUE, max.compute = T, min.compute = T, rand.compute = T)
pr_auc.lda <- as.numeric(pr_curve_glm_lda$auc.integral)

new_row <- data.frame(Model_name = Model_name,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      PR_AUC = pr_auc.lda)

df_metrics <- rbind(df_metrics, new_row)
```

## QDA

Quadratic Discriminant Analysis is a classification model that exploits a quadratic decision boundary to separate the two classes of data. It is based on two main assumptions: 

- Each class is drawn from a normal distribution

- Each class has a different covariance matrix.

The first assumptions is shared with LDA model: we have already proved that the dataset doesn't follow a normal distribution. However, as we did with LDA, we are going to evaluate this model anyway, aware of its limitations due to the conditions not being satisfied.

### QDA with Unbalanced Dataset

We start with the unbalanced dataset. It yields the same results regardless of the threshold used.

```{r}
qda.fit <- qda(Attrition ~ .  - JobLevel - YearsAtCompany, data = train.data)
qda.pred <- predict(qda.fit, test.data, type="response")
qda.res <- qda.pred$posterior


qda.pred.best <- as.factor(ifelse(qda.res[,2] > 0.4, 1, 0))
qda.conf.mat <- t(table(test.data$Attrition, qda.pred.best))
print(qda.conf.mat)
compute_metrics(qda.conf.mat)
qda.error <-  mean(test.data$Attrition != qda.pred.best)
print(qda.error)

```

We obtain 21% mean error. It is visible that, regardless of the thresholds, a high *specificity* is achieved: the models barely misclassify the employees who don't plan to leave the workplace. At the same time, the quite high *precision* guarantees that most of the employees predicted as leaving actually leave.

### QDA with Oversampling

```{r}
qda.fit <- qda(Attrition ~ . - JobLevel - YearsAtCompany, data = over.train.data)
qda.pred_over <- predict(qda.fit, test.data, type="response")
qda.res_over <- qda.pred_over$posterior

qda.pred.best <- as.factor(ifelse(qda.res_over[,2] > 0.4, 1, 0))
qda.conf.mat <- table(test.data$Attrition, qda.pred.best)
print(t(qda.conf.mat))
metrics.qda<-compute_metrics(t(qda.conf.mat))
qda.error <-  mean(test.data$Attrition != qda.pred.best)
print(qda.error)
```

```{r, echo=FALSE, message=FALSE}
Model_name <- "QDA"

accuracy <- as.numeric(metrics.qda[2, which(metrics.qda[1, ] == "Accuracy")])
precision <- as.numeric(metrics.qda[2, which(metrics.qda[1, ] == "Precision")])
recall <- as.numeric(metrics.qda[2, which(metrics.qda[1, ] == "Recall")])
specificity <- as.numeric(metrics.qda[2, which(metrics.qda[1, ] == "Specificity")])
f1_score <- as.numeric(metrics.qda[2, which(metrics.qda[1, ] == "F1 score")])

pr_curve_glm_qda<- pr.curve(scores.class0 = qda.pred.best[class0_indices], scores.class1 = qda.pred.best[-class0_indices], curve=TRUE, max.compute = T, min.compute = T, rand.compute = T)
pr_auc.qda <- as.numeric(pr_curve_glm_qda$auc.integral)

new_row <- data.frame(Model_name = Model_name,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      PR_AUC = pr_auc.qda)

df_metrics <- rbind(df_metrics, new_row)
```


## KNN

KNN is an instance-based learning algorithm that is based on the assumptions that examples with similar features belong to the same class. Given an unseen example, the model assigns the label of the majority class among the K nearest neighbors. Since it involves the computation of distances between data examples, it is necessary to scale data so that features with a largest magnitude don't dominate. 


```{r}
n <- nrow(HR_Analytics)
scaled.data <- as.data.frame(scale(HR_Analytics[-2]))
set.seed(108)
train <- sample(1:n, round(n * 0.75))
test <- setdiff(1:n, train)

train_data <- scaled.data[train, ]
train_data$Attrition <- HR_Analytics[train, 2]
test_data <- scaled.data[test, ]
test_data$Attrition <- HR_Analytics[test, 2]

over.train_data <- ovun.sample(Attrition ~ ., 
                               data = train_data, p = 0.5, method = "over", seed = 108)$data

best_f1_score <- 0  # Initialize with a low value
best_k <- 0
consecutive_increases <- 0  # Initialize a counter for consecutive increases

for (K in 3:50) {
  cat("\nK =", K)
  predicted.classes <- knn(over.train_data, test_data, over.train_data$Attrition, K)
  conf.matrix <- table(predicted.classes, test_data$Attrition)
  metrics <- compute_metrics(conf.matrix)
  current_f1_score <- metrics[2, which(metrics[1, ] == "F1 score")]
  
  if (current_f1_score > best_f1_score) {
    best_f1_score <- current_f1_score
    best_k <- K
    consecutive_increases <- 0  # Reset the counter
  } else {
    consecutive_increases <- consecutive_increases + 1
    if (consecutive_increases >= 5) {  # Adjust the number of consecutive increases as needed
      break
    }
  }
}

cat("\nBest K value:", best_k)

# Now, use the best K value to make predictions
knn.pred <- knn(over.train_data, test_data, over.train_data$Attrition, best_k)
conf.matrix <- table(knn.pred, test_data$Attrition)
conf.matrix
metrics.knn <- compute_metrics(conf.matrix)

```

```{r, echo=FALSE, message=FALSE}
Model_name <- "KNN"
#cm <- confusionMatrix(data=as.factor(predicted.classes.over), reference = test.data$Attrition)

accuracy <- as.numeric(metrics.knn[2, which(metrics.knn[1, ] == "Accuracy")])
precision <- as.numeric(metrics.knn[2, which(metrics.knn[1, ] == "Precision")])
recall <- as.numeric(metrics.knn[2, which(metrics.knn[1, ] == "Recall")])
specificity <- as.numeric(metrics.knn[2, which(metrics.knn[1, ] == "Specificity")])
f1_score <- as.numeric(metrics.knn[2, which(metrics.knn[1, ] == "F1 score")])

pr_curve_knn<- pr.curve(scores.class0 = knn.pred[class0_indices], scores.class1 = knn.pred[-class0_indices], curve=TRUE, max.compute = T, min.compute = T, rand.compute = T)
pr_auc.knn <- as.numeric(pr_curve_knn$auc.integral)

new_row <- data.frame(Model_name = Model_name,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      PR_AUC = pr_auc.knn)

df_metrics <- rbind(df_metrics, new_row)
```


We are only showing a few instances of KNN that evidently demonstrate the behaviour of the model: *specificity* and *accuracy* improve with K, which is an hyperparameter. In fact, since our test data is highly imbalanced, as K increases, it becomes easier for the model to classify any unseen data point as the majority class label. 
```{r echo=FALSE, fig.height=4, message=FALSE}

test.data$pred <- predicted.classes

test.data$pred <- predicted.classes
test.data$Attrition <- factor(test.data$Attrition)

ggarrange(
    ggplot(test.data, aes(BusinessTravel, OverTime)) + 
      geom_jitter(aes(color = pred), size = 2, alpha=0.6) +
      scale_fill_manual(values=custom_colorsD) +
      scale_colour_manual(values=custom_colorsD) +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 0.9)),
    
        ggplot(test.data, aes(BusinessTravel, OverTime)) + 
      geom_jitter(aes(color = test.data$Attrition), size = 2, alpha=0.6) +
      scale_fill_manual(values=custom_colorsD) +
      scale_colour_manual(values=custom_colorsD) +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 0.9)),
    
    ncol=2,
    common.legend = TRUE)
```

## Comparison between the models

```{r, echo=FALSE, fig.align = "center"}
sorted_df <- df_metrics[order(-df_metrics$Recall, -df_metrics$F1_score), ]
rounded_df <- sorted_df[, -ncol(sorted_df)]
kable(rounded_df, format = "markdown", row.names = FALSE)

```

```{r, echo=FALSE, fig.align = "center"}
plot(pr_curve_glm, max.plot = TRUE, min.plot = TRUE, rand.plot = TRUE, fill.area = TRUE, color = 2, auc.main = FALSE, ylim = c(0.0, 1))
plot(pr_curve_glm_step, add = TRUE, color = 3)
plot(pr_curve_glm_ridge, add = TRUE, color = 4)
plot(pr_curve_glm_lasso, add = TRUE, color = 5)
plot(pr_curve_glm_unb, add = TRUE, color = 6)
plot(pr_curve_glm_lda, add=TRUE, color=7)
legend_text <- c(
  paste0("LR (AUC = ", round(pr_curve_glm$auc.integral, 4), ")"),
  paste0("LR with Backward(AUC = ", round(pr_curve_glm_step$auc.integral, 4), ")"),
  paste0("Lasso (AUC = ", round(pr_curve_glm_lasso$auc.integral, 4), ")"),
  paste0("Ridge (AUC = ", round(pr_curve_glm_ridge$auc.integral, 4), ")"),
  paste0("LR Unb (AUC = ", round(pr_curve_glm_ridge$auc.integral, 4), ")"),
  paste0("LDA (AUC = ", round(pr_curve_glm_lda$auc.integral, 4), ")")
)

legend("bottomright", legend = legend_text, col = c(2, 3, 4, 5, 6, 7,8,9), lty = 1, lwd = 2, bty = "n", cex = 0.8)
# round(pr_curve_glm$auc.integral, 4)
```

The plot above explicitly states that the reported models present a similar behaviour: the AUC (Area Under Curve) doesn't overly different among the different models. 

# Misclassification Analysis
We proceeded to analyze the misclassified instances for the logistic regression model after applying backward selection. In general, our models, with the right threshold, managed to achieve a good recall value, with a low number of misclassified false negatives but a higher number of false positives.
Observing the following plots, it is noticeable that the instances misclassified the most are those belonging to categories with a higher probability of Attrition, as we have seen in the EDA.  We showed a tendency to leave their work among employees who are single, work overtime, or travel frequently for work. Analyzing the misclassified instances, it's evident that employers in these categories who don't quit are more challenging to classify correctly, as they share many patterns with their colleagues who do leave. In the following plots, we observe this behavior for the features *MaritalStatus*, *OverTime*, and *BusinessTravel*.

```{r include=FALSE}
library(tidyr)
```

```{r echo=TRUE}
test.data <- test.data %>%
  mutate(Misclassified = ifelse(predicted.classes.over != test.data$Attrition, 1, 0),
         MisclassifiedFP = ifelse(predicted.classes.over == 1 & test.data$Attrition == 0, 1, 0),
         MisclassifiedFN = ifelse(predicted.classes.over== 0 & test.data$Attrition == 1, 1, 0))


```

```{r echo=TRUE}

marital_stats <- test.data %>%
  group_by(MaritalStatus) %>%
  summarise(CorrectPercentage = sum(1 - Misclassified) / n() * 100,
            MisclassifiedFPPercentage = sum(MisclassifiedFP) / n() * 100,
            MisclassifiedFNPercentage = sum(MisclassifiedFN) / n() * 100) %>%
  pivot_longer(cols = c("CorrectPercentage", "MisclassifiedFPPercentage", "MisclassifiedFNPercentage"),
               names_to = "PercentageType", values_to = "Percentage")


marital_stats$MaritalStatus <- factor(marital_stats$MaritalStatus, levels = c(1, 0, 2),
                                      labels = c("Single", "Married", "Divorced"))


ggplot(data = marital_stats, aes(x = MaritalStatus, y = Percentage, fill = PercentageType)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(title = "Percentage of Correctly and Misclassified Instances by MaritalStatus",
       x = "MaritalStatus", y = "Percentage") +
  scale_fill_manual(values = c("CorrectPercentage" = "#7FB942", "MisclassifiedFPPercentage" = "#B9427F", "MisclassifiedFNPercentage" = "#427FB9")) +
  theme_minimal() +
  theme(legend.position = "top")
```
```{r echo=FALSE}

overtime_stats <- test.data %>%
  group_by(OverTime) %>%
  summarise(CorrectPercentage = sum(1 - Misclassified) / n() * 100,
            MisclassifiedFPPercentage = sum(MisclassifiedFP) / n() * 100,
            MisclassifiedFNPercentage = sum(MisclassifiedFN) / n() * 100) %>%
  pivot_longer(cols = c("CorrectPercentage", "MisclassifiedFPPercentage", "MisclassifiedFNPercentage"),
               names_to = "PercentageType", values_to = "Percentage")

overtime_stats$OverTime <- factor(overtime_stats$OverTime,
                                  levels = c(0, 1),
                                  labels = c("No", "Yes"))


ggplot(data = overtime_stats, aes(x = OverTime, y = Percentage, fill = PercentageType)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(title = "Percentage of Correctly and Misclassified Instances by OverTime",
       x = "OverTime", y = "Percentage") +
  scale_fill_manual(values = c("CorrectPercentage" = "#7FB942", "MisclassifiedFPPercentage" = "#B9427F", "MisclassifiedFNPercentage" = "#427FB9")) +
  theme_minimal() +
  theme(legend.position = "top", plot.title = element_text(size = 12))

```

```{r echo=TRUE}

business_travel_stats <- test.data %>%
  group_by(BusinessTravel) %>%
  summarise(CorrectPercentage = sum(1 - Misclassified) / n() * 100,
            MisclassifiedFPPercentage = sum(MisclassifiedFP) / n() * 100,
            MisclassifiedFNPercentage = sum(MisclassifiedFN) / n() * 100) %>%
  pivot_longer(cols = c("CorrectPercentage", "MisclassifiedFPPercentage", "MisclassifiedFNPercentage"),
               names_to = "PercentageType", values_to = "Percentage")



business_travel_stats$BusinessTravel <- factor(business_travel_stats$BusinessTravel,
                                               levels = c(0, 1, 2),
                                               labels = c("Non-Travel", "Travel_Rarely", "Travel_Frequently"))


ggplot(data = business_travel_stats, aes(x = BusinessTravel, y = Percentage, fill = PercentageType)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(title = "Percentage of Correctly and Misclassified Instances by BusinessTravel",
       x = "BusinessTravel", y = "Percentage") +
  scale_fill_manual(values = c("CorrectPercentage" = "#7FB942", "MisclassifiedFPPercentage" = "#B9427F", "MisclassifiedFNPercentage" = "#427FB9")) +
  theme_minimal() +
  theme(legend.position = "top", plot.title = element_text(size = 12))
```

Employees who both travel frequently for work and work overtime are more likely to be misclassified.
```{r echo=FALSE}

combined_stats <- test.data %>%
  group_by(BusinessTravel, OverTime) %>%
  summarise(CorrectPercentage = sum(1 - Misclassified) / n() * 100,
            MisclassifiedFPPercentage = sum(MisclassifiedFP) / n() * 100,
            MisclassifiedFNPercentage = sum(MisclassifiedFN) / n() * 100) %>%
  pivot_longer(cols = c("CorrectPercentage", "MisclassifiedFPPercentage", "MisclassifiedFNPercentage"),
               names_to = "PercentageType", values_to = "Percentage")


combined_stats$BusinessTravel <- factor(combined_stats$BusinessTravel,
                                        levels = c(0, 1, 2),
                                        labels = c("Non-Travel", "Travel_Rarely", "Travel_Frequently"))


combined_stats$OverTime <- factor(combined_stats$OverTime,
                                  levels = c(0, 1),
                                  labels = c("No", "Yes"))


ggplot(data = combined_stats, aes(x = BusinessTravel, y = Percentage, fill = PercentageType)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(title = "Percentage of Correctly and Misclassified Instances by BusinessTravel and OverTime",
       x = "BusinessTravel", y = "Percentage") +
  scale_fill_manual(values = c("CorrectPercentage" = "#7FB942", "MisclassifiedFPPercentage" = "#B9427F", "MisclassifiedFNPercentage" = "#427FB9")) +
  theme_minimal() +
  theme(legend.position = "top", plot.title = element_text(size = 12)) +
  facet_grid(. ~ OverTime)

```


In our exploratory data analysis, we observed that employees in managerial and research director roles tend to have longer tenures with the company and receive higher salaries. Consequently, they are less likely to leave their positions and are also less prone to misclassification. In contrast, sales representatives and HR_Analytics staff exhibit a different pattern, with a higher likelihood of both attrition and misclassification.

```{r echo=FALSE}

job_role_stats <- test.data %>%
  group_by(JobRole) %>%
  summarise(CorrectPercentage = sum(1 - Misclassified) / n() * 100,
            MisclassifiedFPPercentage = sum(MisclassifiedFP) / n() * 100,
            MisclassifiedFNPercentage = sum(MisclassifiedFN) / n() * 100) %>%
  pivot_longer(cols = c("CorrectPercentage", "MisclassifiedFPPercentage", "MisclassifiedFNPercentage"),
               names_to = "PercentageType", values_to = "Percentage")


job_role_stats$JobRole <- factor(job_role_stats$JobRole,
                                 levels = c(0, 1, 2, 3, 4, 5, 6, 7, 8),
                                 labels = c("Sales Executive", "Research Scientist", "Laboratory Technician", "Manufacturing Director", "Healthcare Representative", "Manager", "Sales Representative", "Research Director", "Human Resources"))


ggplot(data = job_role_stats, aes(x = JobRole, y = Percentage, fill = PercentageType)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(title = "Percentage of Correctly and Misclassified Instances by JobRole",
       x = "JobRole", y = "Percentage") +
  scale_fill_manual(values = c("CorrectPercentage" = "#7FB942", "MisclassifiedFPPercentage" = "#B9427F", "MisclassifiedFNPercentage" = "#427FB9")) +
  theme_minimal() +
  theme(legend.position = "top", axis.text.x = element_text(angle = 45, hjust = 1), plot.title =element_text(size = 12))
```

We then proceeded to visualize the distribution of monthly income, stratified into the correctly classified and misclassified.


```{r echo=FALSE}
FP <- (predicted.classes.over == 1) & (test.data$Attrition == 0)
FN <- (predicted.classes.over == 0) & (test.data$Attrition == 1)
Correct<-predicted.classes.over == test.data$Attrition
MisclassifiedFP<- test.data[FP,]
MisclassifiedFN<- test.data[FN,]
Correct<- test.data[Correct,]


combined_data <- rbind(
  data.frame(Category = "MisclassifiedFP", MonthlyIncome = MisclassifiedFP$MonthlyIncome),
  data.frame(Category = "MisclassifiedFN", MonthlyIncome = MisclassifiedFN$MonthlyIncome),
  data.frame(Category = "Correct", MonthlyIncome = Correct$MonthlyIncome)
)


palette <- c("MisclassifiedFP" = "#B9427F", "MisclassifiedFN" = "#427FB9", "Correct" = "#7FB942")

ggplot(data = combined_data, aes(x = MonthlyIncome, color = Category, fill = Category)) +
  geom_density(alpha = 0.7) +
  scale_color_manual(values = palette) +
  scale_fill_manual(values = palette) +
  labs(title = "Line Density of Monthly Income by Category",
       x = "Monthly Income") +
  theme_minimal()

```

# Conclusions


We realized that our models have quite similar metrics and a tendency to overfit the test data. Ultimately, we decided to opt for the model with the highest recall - which is Lasso Regression - as our goal is to identify employees who may be considering leaving the company. Lasso Regression not only has the highest recall but also maintains a good balance with the other metrics, boosting the biggest area under the precision-recall curve.
The actions the company should take to prevent attrition should not harm the employees who our model misclassifies, so it's important to make these investments wisely.
From the chosen model, it is evident that the most significant feature is *Overtime*. We recommend the company take this into consideration by potentially adjusting the overtime rates and establishing limits on weekly overtime hours. Furthermore, it's crucial to gain a deeper understanding of the underlying factors contributing to this phenomenon. One way to do this is by conducting surveys to comprehend the workflow within the company. This survey can help identify if there are significant factors, especially related to teamwork, that may be slowing down productivity.
It's also essential to determine whether employees are voluntarily opting to work overtime due to financial needs or if it's a result of a stressful or unhealthy work environment.
The second most influential variable is *BusinessTravel*: undoubtely, the most intuitive action a company can take is to guarantee a better compensation for travel expenses.
Nevertheless, our final suggestion for any company would be to submit periodic surveys in order to have a constantly updated picture of the well-being and intentions of the employees allowing to have an accurate model prediction. Obviously, such predictions must be continuously taken in consideration so that timely measures can be adopted to limit any loss.
We further propose to integrate more specific questions into the survey:

- *ManagerSatisfaction*: evaluation from 1 to 4 of the manager;

- *TeamWork*: evaluation from 1 to 4 of the quality of the environment and workflow in the team.